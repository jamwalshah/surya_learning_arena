# Review

ðŸŽ‰ Congratulations on reaching the end of the module! Take a moment to review everything you have learned:

## Airflow Architecture

- Airflow has six main components:
  - The web server for serving and updating the Airflow user interface.
  - The metadata database for storing all metadata (e.g., users, tasks) related to your Airflow instance.
  - The scheduler for monitoring and scheduling your pipelines.
  - The executor for defining how and on which system tasks are executed.
  - The queue for holding tasks that are ready to be executed.
  - The worker(s) for executing instructions defined in a task.

- Airflow runs DAGs in six different steps:
  - The scheduler constantly scans the DAGs directory for new files. The default time is every 5 minutes.
  - After the scheduler detects a new DAG, the DAG is processed and serialized into the metadata database.
  - The scheduler scans for DAGs that are ready to run in the metadata database. The default time is every 5 seconds.
  - Once a DAG is ready to run, its tasks are put into the executor's queue.
  - Once a worker is available, it will retrieve a task to execute from the queue.
  - The worker will then execute the task.

## DAG Components

- A DAG has four core parts:
  - The import statements, where the specific operators or classes that are needed for the DAG are defined.
  - The DAG definition, where the DAG object is called and where specific properties, such as its name or how often you want to run it, are defined.
  - The DAG body, where tasks are defined with the specific operators they will run.
  - The dependencies, where the order of execution of tasks is defined using the right bitshift operator (>>) and the left bitshift operator (<<).

- A DAGâ€™s task can be defined as being either upstream or downstream to another task.
