# 02. Machine Learning - Statistics - Bayes' Rules, Conditional Probability, Chain Rule

- Now we're equipped with the ability to calculate the probability of events when they are not dependent on any other events around them
- But this definitely creates a practical limitation as many events are contingent on each other in reality
- Further dealing with Conditional Probability and Bayes' Theorem, we'll answer these limitations

## Conditional Probability

- As the name suggests, **Conditional Probability** comes into play when the probability of occurrence of a particular event changes when one or more conditions are satisfied (these conditions again are events)
- Speaking in technical terms, if **X** and **Y** are two events, then the **Conditional Probability of X w.r.t. Y** is denoted by

  ${P(X|Y)}$

- So, when we talk in terms of Conditional Probability, just for an example, we make a statement like
  
  > The Probability of event X given that Y has already occurred

## What if X and Y are independent events?

- From the definition of independent events, the occurrence of event **X** is not dependent on event **Y**
- Therefore ${P(X|Y) = P(X)}$

## What if X and Y are mutually exclusive?

- As X and Y are disjoint events, the probability that event **X** will occur when event **Y** has already occurred is **0**
- Therefore ${P(X|Y) = 0}$

## Product Rule

- Product Rule states that

  ${P(X \cap Y) = P(X|Y) * P(Y)}$

  - ${P(Y)}$: Probability that event **Y** occurs
  - ${P(X|Y)}$: Probability that event **X** occurs given that event **Y**  has already occurred
- This product rule can be presented as

  ${P(X | Y) = \frac{P(X \cap Y)}{P(Y)}}$
  - ${P(X | Y)}$: Probability of event **X** given event **Y** has already occurred
  - ${P(X \cap Y)}$: Probability of event **X** and event **Y**
  - ${P(Y)}$: Probability of event **Y**
- So, the joint probability that both event **X** and event **Y** will occur is equal to the product term of two terms
  - Probability that event **Y** occurs
  - Probability that event **X** occurs given that event **Y**  has already occurred
- From the Product Rule, the following can be inferred
  1. ${X \subseteq Y \implies P(X|Y) = \frac{P(X \cap Y)}{P(Y)} = \frac{P(X)}{P(Y)}}$
      - Because ${X \subseteq Y \implies X \cap Y = X}$
  2. ${Y \subseteq X \implies P(X|Y) = \frac{P(X \cap Y)}{P(Y)} = \frac{P(Y)}{P(Y)} = 1}$
      - Because ${Y \subseteq X \implies X \cap Y = Y}$

## Validity of Laws of Probability

- The distributive, associative and DeMorgans laws are valid for probability calculations too
- The following can be inferred
  1. ${P(X \cup Y|Z) = P(X|Z) + P(Y|Z) - P(X \cap Y|Z)}$
  2. ${P(X^c|Z) = 1 - P(X|Z)}$

## Chain Rule

- Generalizing the Product Rule leads to the Chain Rule
- Let ${E_1, E_2, ..., E_n}$ be **n** events
- THe joint probability of all the **n** events is given by

  ${P(\cap_{i=1,...,n}E_i) = P(E_n|\cap_{i=1,...,n-1}E_i) * P(\cap_{i=1,...,n-1}E_i)}$

- The Chain Rule can be used iteratively to calculate the joint probability of any number of events

## Bayes' Theorem

- From the Product Rule, we have

  1. ${P(X \cap Y) = P(X|Y) * P(Y)}$

  2. ${P(Y \cap X) = P(Y|X) * P(X)}$

  - Since both ${P(X \cap Y)}$ and ${P(Y \cap X)}$ are same/equal

    ${\implies P(Y \cap X) = P(X \cap Y)}$

    ${\implies P(Y|X) * P(X) = P(X|Y) * P(Y)}$

    ${\implies P(X) = \frac{P(X|Y) * P(Y)}{P(Y|X)}}$
  
  - where ${P(X) = P(X \cap Y) + P(X \cap Y^c)}$ from Sum Rule
