# Azure Databricks

## Azure Databricks Beginner-Level

1. **What is Azure Databricks?**
    - Azure Databricks is a cloud-based big data analytics and processing platform provided by Microsoft Azure.
    - It simplifies the process of building, managing, and scaling big data analytics and machine learning workflows in the Azure cloud infrastructure.

2. **What are the main components of Azure Databricks?**
    - The main components of Azure Databricks include :
        - Collaborative Workspace
        - Managed Infrastructure
        - Spark
        - Delta
        - MLflow
        - SQL Analytics

3. **What is Reserved Capacity in Azure?**
    - **Azure reserved capacity** offers discounted prices compared to pay-as-you-go pricing, allowing you to save money by committing upfront to a certain quantity of resources for one or three years. It works well for workloads that are predictable and is available for services including virtual machines (VMs), Azure SQL Database, and Cosmos DB.

4. **Describe the Various Components That Make Up Azure Synapse Analytics.**
    - Azure Synapse Analytics combines the following elements:
        - SQL Data Warehouse
        - Apache Spark
        - Azure Data Lake Storage
        - Azure Data Factory
        - Power BI

5. **Give Some Advantages of Azure Databricks**
    - The **advantages** of Azure Databricks are as follows:
        - Collaborative Environment
        - Scalability
        - Time-to-Value
        - Security

6. **What Does "Auto-Scale" Mean in the Context of Azure Databricks When It Comes to a Cluster of Nodes?**
    - **"Auto-scaling"** refers to the ability of a cluster to automatically adjust the number of worker nodes based on the workload or the amount of data being processed
    - If the workload increases, the cluster can automatically add more worker nodes to handle the load and remove worker nodes when not needed

7. **Explain the Function of the Databricks File System**
    - The **Databricks File System (DBFS)** is a distributed file system that provides a unified storage layer for data in **Azure Databricks**. It allows users to easily access and share files across clusters, notebooks, and jobs, providing a scalable and reliable way to manage data for analytics and machine learning tasks.

8. **Name Different Types of Clusters Present in Azure Databricks.**
    - **Single-Node Clusters :** ideal for beginners, one-machine cluster perfect for testing code snippets & prototyping
    - **Multi-Node Clusters :** goto option for massive datasets, complex data analysis & running intricate algorithms, combines power of multiple machines for efficient processing of large data volumes
    - **Auto-Scaling Clusters :** is a dynamic option which has multi-node family adjusting their sizes according to workload, in case fluctuating processing demands, it ensures that you have resources without manual interference, optimizes the cost efficiency
    - **High Concurrency Clusters :** concurrency means repetition, collaboration is the dey designed in it for shared use of multiple users, prioritizes efficient resource allocation to handle concurrent queries from various users without compromising performance, ideal for team-based data exploration and analysis
    - **GPU-Enabled Clusters :** has GPUs involved, designed specifically for computationally intensive workloads like Machine Learning, GPUs accelerate these tasks significantly as compared to traditional CPUs

9. **How Does Azure Databricks Handle Security?**
    - Azure Active Directory Integration : user-access management achieved through seamless integration with Azure Active Directory, eliminates the need for separate credentials for Databricks, allows users to sign-in with their existing organization ID, is also referred as Single-Sign-On (SSO), reduces the risk of unauthorized login attempts
    - Network Security : Granular access over network is provided, Network admins can define specific IP addresses or ranges whitelisted to access Databricks workspaces, thus restricts access from unauthorized locations
    - Role-Based Access Control (RBAC) : empowers admins to define user permissions within Databricks environment, ensures that users only have access to resources & functionalities that are needed, follows a principle of least privilege minimizes potential damage in case of compromised accounts
    - Cluster Isolation : network policy imposed by isolating workspaces within VMs or VNets, creates a secure environment for your data processing tasks & separates you from public internet and other resources
    - Data Encryption : Data is protected at both rest and in transit through robust encryption, databases are secured using industry-standard algorithms & data transmission is secured using TLS or SSL protocols, ensures even if some data is intercepted our data remains unreadable
    - Audit Logging and Monitoring : creating comprehensive audit logs that tracks all your user actions within the workspace, allows admins to monitor activity and identify potential security breaches which are suspicious behavior happening in databricks account, also integrates with Azure monitoring services that further provide insights on resource utilization
    - Secrets Management : is a sensitive information like passwords or API keys that're often used within the notebooks, Databricks integrates with Azure Key Vault which is a secure store for managing all these secrets

10. **What Do You Understand by the Term "Control Plane"?**
    - The **control plane** refers to the management plane, where Databricks operates the workspace application and manages configuration, notebooks, libraries, and clusters. In Databricks, the workspace application for managing notebooks, setups, and clusters is located on the control plane, which also acts as the administration center.
    - It has following parts
      1. **Web Application :** represents a UI or API through which users interact with the system
      2. **Repos/Notebooks :** users can submit data processing tasks such as configure settings & monitor the progress of their jobs also
      3. **Job Scheduling :** responsible for managing the execution of data processing tasks, receives requests from Web Application and translates them into instructions for the data and schedules their execution
      4. **Cluster Management :** serverless processing takes place, and it also eliminates manual cluster management, some implementations might have basic cluster management component while others cannot have that, also responsible for provisioning resources for stateful services or some specific configurations that are required by some of the data processing tasks

---

## Azure Databricks Intermediate-Level

1. **Which Are Some of the Most Important Applications of Kafka in Azure Databricks?**
    - The **applications of Kafka** in Azure Databricks include:
        - Real-Time Data Processing : streamlines data from Kafka instantly using Spark Streaming, creates a data pipeline reliably moves data from one system to another, to gain valuable insights from data as it arrives allowing for faster decision making
        - Data Integration : also called as Unified Data Pipeline, helps you to integrate data from various sources into Databricks using Kafka streams, simplifies data ingestion and helps you to secure & build data pipelines for comprehensive analysis
        - Event-Driven Architecture : basically a reactive architecture that responds swiftly to data changes and user interactions that are published through Kafka Spark Streaming in Databricks, allows to react to these events in real-time
        - Microservices Communication : Microservices has been made easy, also facilitates communication between independent microservices running on Databricks or other cloud platforms
    - It promotes modularity & scalability to your App by leveraging all of these applications within Kafka Azure Databricks together
    - You can build a powerful data processing environments that thrives on the real-time

2. **Explain the Difference Between Azure Databricks and AWS Databricks.**

    | Aspects | Azure Databricks | AWS Databricks |
    | :- | :- | :- |
    | **Cloud Provider** | Microsoft Azure | Amazon Web Services (AWS) |
    | **Integration** | Deep integration with Azure services like ADLS, SQL DW, and more | Integration with AWS services like S3, Redshift, Glue, AWS SageMaker etc |
    | **Security** | Integrated with Azure Active Directory for authentication | Integrated with AWS IAM for authentication and access control |
    | **Machine Learning** | Integration with Azure Machine Learning for ML workflows | Integration with ML tasks and model deployment |
    | **Analytics Tools** | Integration with Azure Data Factory, Power BI, and more | Integration with AWS Glue, Athena, QuickSight, and others |

3. **Name the Types of Widgets Used in Azure Databricks.**
    - Input widgets allow you to add parameters to your notebooks and dashboards.
    - You can add a widget from the Databricks UI or using the widget API.
    - To add or edit a widget, you must have CAN EDIT permissions on the notebook.
    - There are four types of widgets available in Azure Databricks:
        - `text` : Input a value in a text box.
        - `dropdown` : Select a value from a list of provided values.
        - `combobox` : Combination of text and dropdown. Select a value from a provided list or input one in the text box.
        - `multiselect` : Select one or more values from a list of provided values.

4. **What is a Delta table?**
    - Any table that contains data contained in Delta format is referred to as Delta table
    - On top of Apache Spark, these tables offer ACID properties (Atomicity, Consistency, Isolation & Durability)
    - Delta tables have schema evolution, means when new columns are added to the table, it does not need manual schema modification, as schema evolves, current queries run well
    - Increases the flexibility and the speed of data-pipelines while also allowing them to be customized to meet changing requirements

5. **How Does Azure Databricks Handle Schema Evolution in Delta Tables?**
    - In its simplest form, automatic schema evolution simply implies that when new columns are inserted into the data, it does not need manual schema modifications.
    - As the schema progresses, current queries run well, and virus-like operating scheme changes are easily handled.

6. **Explain the Concept of Table Caching in Azure Databricks.**
    - Table caching in Azure Databricks is like keeping a quick-access copy of a table or data in the computer's memory. As a result, the data may be searched and analyzed much faster because it is not constantly read from storage. It's like having a reference cheat sheet to speed up time-consuming tasks like sorting, finding, and performing calculations.

7. **Explain Azure Data Lakehouse and Data Lake**
    - First we have Data Warehouse, then we have Data Lake, and then we have Data Lakehouse, and all these three are connected to each other
    - Azure Data Lakehouse combines the features of a Data Lake with the capabilities of a Data Warehouse
      - It provides a platform foe managing both structured and unstructured data, bridging the gap between traditional Data Warehouses and Data Lakes
      - It is a highly scalable and secured data storage solution offered by Microsoft Azure
      - Large volumes of both structured and unstructured data can be stored by businesses in their original form
      - High Productivity, smooth access control and interaction with several analytics and data processing services are just few of the benefits of the Azure Data Lake provides
      - Azure Data Lakehouse is a combination of Data Warehouse and Data Lake
    - Data Lake has the features like ACID transactions, schema enforcement, which are also present in Data Lakehouse
      - It is designed for Big Data Analytics applications
    - In Data Warehouse, only structured data is present along with the tools like BI and reporting

8. **What is the difference between a Data Warehouse and a Data Lake**

    | Aspect | Data Warehouse | Data Lake |
    | :-- | :-- | :-- |
    | Data Type | Primarily structured data | Structured, semi-structured and unstructured data |
    | Schema | Schema-on-write (rigid schema enforced before storage) | Schema-on-read (schema flexibility, applied when reading data) |
    | Processing Speed | Optimized for high-speed read-heavy workloads | Flexible, can handle batch, streaming and ad-hoc processing |
    | Data Storage | Normalized and structured storage | Raw, as-is storage preserving original data format |
    | Cost | Usually higher due to structured and indexed storage | Often lower due to storage optimization and flexible schema |

9. What are PySpark Data Frames?

    - In Apache Spark, PySpark Data Frames are distributed collections of data organized into named columns, similar to traditional tables in databases and spreadsheets.
    - Some of the key characteristics include:
        - Distributed
        - Structured
        - Lazy Evaluation

10. as
