# Azure Databricks

## Azure Databricks Beginner-Level

Q1. **What is Azure Databricks?**

- Azure Databricks is a cloud-based big data analytics and processing platform provided by Microsoft Azure.
- It simplifies the process of building, managing, and scaling big data analytics and machine learning workflows in the Azure cloud infrastructure.

Q2. **What are the main components of Azure Databricks?**

- The main components of Azure Databricks include
  - Collaborative Workspace
  - Managed Infrastructure
  - Spark
  - Delta
  - MLflow
  - SQL Analytics

Q3. **What is Reserved Capacity in Azure?**

- **Azure reserved capacity** offers discounted prices compared to pay-as-you-go pricing, allowing you to save money by committing upfront to a certain quantity of resources for one or three years. It works well for workloads that are predictable and is available for services including virtual machines (VMs), Azure SQL Database, and Cosmos DB.

Q4. **Describe the Various Components That Make Up Azure Synapse Analytics.**

- Azure Synapse Analytics combines the following elements:
  - SQL Data Warehouse
  - Apache Spark
  - Azure Data Lake Storage
  - Azure Data Factory
  - Power BI

Q5. **Give Some Advantages of Azure Databricks**

- The **advantages** of Azure Databricks are as follows:
  - Collaborative Environment
  - Scalability
  - Time-to-Value
  - Security

Q6. **What Does "Auto-Scale" Mean in the Context of Azure Databricks When It Comes to a Cluster of Nodes?**

- **"Auto-scaling"** refers to the ability of a cluster to automatically adjust the number of worker nodes based on the workload or the amount of data being processed
- If the workload increases, the cluster can automatically add more worker nodes to handle the load and remove worker nodes when not needed

Q7. **Explain the Function of the Databricks File System**

- The **Databricks File System (DBFS)** is a distributed file system that provides a unified storage layer for data in **Azure Databricks**. It allows users to easily access and share files across clusters, notebooks, and jobs, providing a scalable and reliable way to manage data for analytics and machine learning tasks.

Q8. **Name Different Types of Clusters Present in Azure Databricks.**

- **Single-Node Clusters :** ideal for beginners, one-machine cluster perfect for testing code snippets & prototyping
- **Multi-Node Clusters :** goto option for massive datasets, complex data analysis & running intricate algorithms, combines power of multiple machines for efficient processing of large data volumes
- **Auto-Scaling Clusters :** is a dynamic option which has multi-node family adjusting their sizes according to workload, in case fluctuating processing demands, it ensures that you have resources without manual interference, optimizes the cost efficiency
- **High Concurrency Clusters :** concurrency means repetition, collaboration is the dey designed in it for shared use of multiple users, prioritizes efficient resource allocation to handle concurrent queries from various users without compromising performance, ideal for team-based data exploration and analysis
- **GPU-Enabled Clusters :** has GPUs involved, designed specifically for computationally intensive workloads like Machine Learning, GPUs accelerate these tasks significantly as compared to traditional CPUs

Q9. **How Does Azure Databricks Handle Security?**

- *Azure Active Directory Integration* : user-access management achieved through seamless integration with Azure Active Directory, eliminates the need for separate credentials for Databricks, allows users to sign-in with their existing organization ID, is also referred as Single-Sign-On (SSO), reduces the risk of unauthorized login attempts
- *Network Security* : Granular access over network is provided, Network admins can define specific IP addresses or ranges whitelisted to access Databricks workspaces, thus restricts access from unauthorized locations
- *Role-Based Access Control (RBAC)* : empowers admins to define user permissions within Databricks environment, ensures that users only have access to resources & functionalities that are needed, follows a principle of least privilege minimizes potential damage in case of compromised accounts
- *Cluster Isolation* : network policy imposed by isolating workspaces within VMs or VNets, creates a secure environment for your data processing tasks & separates you from public internet and other resources
- *Data Encryption* : Data is protected at both rest and in transit through robust encryption, databases are secured using industry-standard algorithms & data transmission is secured using TLS or SSL protocols, ensures even if some data is intercepted our data remains unreadable
- *Audit Logging and Monitoring* : creating comprehensive audit logs that tracks all your user actions within the workspace, allows admins to monitor activity and identify potential security breaches which are suspicious behavior happening in databricks account, also integrates with Azure monitoring services that further provide insights on resource utilization
- *Secrets Management* : is a sensitive information like passwords or API keys that're often used within the notebooks, Databricks integrates with Azure Key Vault which is a secure store for managing all these secrets

Q10. **What Do You Understand by the Term "Control Plane"?**

- The **control plane** refers to the management plane, where Databricks operates the workspace application and manages configuration, notebooks, libraries, and clusters. In Databricks, the workspace application for managing notebooks, setups, and clusters is located on the control plane, which also acts as the administration center.
- It has following parts
  1. **Web Application :** represents a UI or API through which users interact with the system
  2. **Repos/Notebooks :** users can submit data processing tasks such as configure settings & monitor the progress of their jobs also
  3. **Job Scheduling :** responsible for managing the execution of data processing tasks, receives requests from Web Application and translates them into instructions for the data and schedules their execution
  4. **Cluster Management :** serverless processing takes place, and it also eliminates manual cluster management, some implementations might have basic cluster management component while others cannot have that, also responsible for provisioning resources for stateful services or some specific configurations that are required by some of the data processing tasks

---

## Azure Databricks Intermediate-Level

Q1. **Which Are Some of the Most Important Applications of Kafka in Azure Databricks?**

- The **applications of Kafka** in Azure Databricks include:
  - *Real-Time Data Processing* : streamlines data from Kafka instantly using Spark Streaming, creates a data pipeline reliably moves data from one system to another, to gain valuable insights from data as it arrives allowing for faster decision making
  - *Data Integration* : also called as Unified Data Pipeline, helps you to integrate data from various sources into Databricks using Kafka streams, simplifies data ingestion and helps you to secure & build data pipelines for comprehensive analysis
  - *Event-Driven Architecture* : basically a reactive architecture that responds swiftly to data changes and user interactions that are published through Kafka Spark Streaming in Databricks, allows to react to these events in real-time
  - *Microservices Communication* : Microservices has been made easy, also facilitates communication between independent microservices running on Databricks or other cloud platforms
- It promotes modularity & scalability to your App by leveraging all of these applications within Kafka Azure Databricks together
- You can build a powerful data processing environments that thrives on the real-time

Q2. **Explain the Difference Between Azure Databricks and AWS Databricks.**

| Aspects | Azure Databricks | AWS Databricks |
| :- | :- | :- |
| **Cloud Provider** | Microsoft Azure | Amazon Web Services (AWS) |
| **Integration** | Deep integration with Azure services like ADLS, SQL DW, and more | Integration with AWS services like S3, Redshift, Glue, AWS SageMaker etc |
| **Security** | Integrated with Azure Active Directory for authentication | Integrated with AWS IAM for authentication and access control |
| **Machine Learning** | Integration with Azure Machine Learning for ML workflows | Integration with ML tasks and model deployment |
| **Analytics Tools** | Integration with Azure Data Factory, Power BI, and more | Integration with AWS Glue, Athena, QuickSight, and others |

Q3. **Name the Types of Widgets Used in Azure Databricks.**

- Input widgets allow you to add parameters to your notebooks and dashboards.
- You can add a widget from the Databricks UI or using the widget API.
- To add or edit a widget, you must have CAN EDIT permissions on the notebook.
- There are four types of widgets available in Azure Databricks:
  - `text` : Input a value in a text box.
  - `dropdown` : Select a value from a list of provided values.
  - `combobox` : Combination of text and dropdown. Select a value from a provided list or input one in the text box.
  - `multiselect` : Select one or more values from a list of provided values.

Q4. **What is a Delta table?**

- Any table that contains data contained in Delta format is referred to as Delta table
- On top of Apache Spark, these tables offer ACID properties (Atomicity, Consistency, Isolation & Durability)
- Delta tables have schema evolution, means when new columns are added to the table, it does not need manual schema modification, as schema evolves, current queries run well
- Increases the flexibility and the speed of data-pipelines while also allowing them to be customized to meet changing requirements

Q5. **How Does Azure Databricks Handle Schema Evolution in Delta Tables?**

- In its simplest form, automatic schema evolution simply implies that when new columns are inserted into the data, it does not need manual schema modifications.
- As the schema progresses, current queries run well, and virus-like operating scheme changes are easily handled.

Q6. **Explain the Concept of Table Caching in Azure Databricks.**

- Table caching in Azure Databricks is like keeping a quick-access copy of a table or data in the computer's memory. As a result, the data may be searched and analyzed much faster because it is not constantly read from storage. It's like having a reference cheat sheet to speed up time-consuming tasks like sorting, finding, and performing calculations.

Q7. **Explain Azure Data Lakehouse and Data Lake**

- First we have Data Warehouse, then we have Data Lake, and then we have Data Lakehouse, and all these three are connected to each other
- Azure Data Lakehouse combines the features of a Data Lake with the capabilities of a Data Warehouse
  - It provides a platform foe managing both structured and unstructured data, bridging the gap between traditional Data Warehouses and Data Lakes
  - It is a highly scalable and secured data storage solution offered by Microsoft Azure
  - Large volumes of both structured and unstructured data can be stored by businesses in their original form
  - High Productivity, smooth access control and interaction with several analytics and data processing services are just few of the benefits of the Azure Data Lake provides
  - Azure Data Lakehouse is a combination of Data Warehouse and Data Lake
- Data Lake has the features like ACID transactions, schema enforcement, which are also present in Data Lakehouse
  - It is designed for Big Data Analytics applications
- In Data Warehouse, only structured data is present along with the tools like BI and reporting

Q8. **What is the difference between a Data Warehouse and a Data Lake**

| Aspect | Data Warehouse | Data Lake |
| :-- | :-- | :-- |
| Data Type | Primarily structured data | Structured, semi-structured and unstructured data |
| Schema | Schema-on-write (rigid schema enforced before storage) | Schema-on-read (schema flexibility, applied when reading data) |
| Processing Speed | Optimized for high-speed read-heavy workloads | Flexible, can handle batch, streaming and ad-hoc processing |
| Data Storage | Normalized and structured storage | Raw, as-is storage preserving original data format |
| Cost | Usually higher due to structured and indexed storage | Often lower due to storage optimization and flexible schema |

Q9. **What are PySpark Data Frames?**

- In Apache Spark, PySpark Data Frames are distributed collections of data organized into named columns, similar to traditional tables in databases and spreadsheets.
- Some of the key characteristics include:
  - *Distributed*
    - Data is stored and processed in parallel across multiple nodes, that enable you to handle massive data sets which would'nt fit on a single machine
  - *Structured*
    - Data is organized in with the rows and named columns, each containing a specific data type like integer, string or date
    - Structure makes it easier to manipulate and analyze data
  - *Lazy Evaluation*
    - Operations on Data Frames are not immediately executed, but are defined in a logical plan
    - When an action happens, like displaying results, then evaluation gets triggered and the plan is to optimize and execute it efficiently

Q10. **What are "Dedicated SQL Pools"?**

- Dedicated SQL Pools are separate compute resource for running SQL queries in Azure Databricks
- They are very useful for the queries that don't require the full power of Databricks cluster
- We have compute nodes, which are further connected to Massively Parallel Processing (MPP) Engine, and then further to Azure Storage

---

## Azure Databricks Advanced-Level

Q1. **What are the different applications of Table Storage in Microsoft Azure?**

- There are many applications of table storage
  - Keeping structured data
    - Structured Data has a freedom
    - Structured data like user preferences, product catalogs, or customer information, without the rigidity of a fixed schema
    - This flexibility caters to a evolving data
  - Web Application
    - Tells you about the web-efficiency that this application provides
    - Powerful web-applications with fast and easy access to large data volume
    - These track user behavior, store session data and manages user profiles efficiently
  - Internet of Things (IoT) and Sensor Data
    - It seamlessly handles the sensor and IoT data from temperature readings to GPS locations
    - Tables storage offers you a scalable solution and diverse device data
  - Analytics and logging
    - It analyzes large data sets and centralizes log data, application logs, web-visitor data, and performance metrics can all be stored for deeper insights in this particular type of application
  - Backup and Disaster Recovery
    - It ensures critical data is safe and you can use Table Storage to create backup that safeguard your information incase of any emergency
- Table storage goes beyond traditional databases by providing a flexible and scalable foundation for various storage data you need

Q2. **How does Azure handle redundant storage of data?**

- To guarantee that data is always accessible, Azure keeps several copies of data at various levels
- Numerous data redundancy techniques are available in Azure Storage facilities that guarantees data security and availability
- Below are four of the redundancy handling techniques
  1. *Locally Redundant Storage (LRS)*
        - Azure copies data across several storage areas stored within the same data center to maintain highly accessible data
        - It is also known as Locally Redundant Storage since the data copies are kept in three separate locations inside the same physical space
        - Multiple replicas across a data center
        - Protect against Disk failures, Node failure, rack failures
        - Write is ACKed when all replicas are committed
        - 11.9s of durability
        - SLA: 99.9%
  2. *Zone Redundant Storage (ZRS)*
        - Data is replicated to three different availability zones of the Primary Zone
        - In this, you can ensure that the data can be recovered from the copies kept at these places if the original site is inaccessible
        - Replicas across 3 Zones
        - Protect against Disk failures, Node failures, Rack failures and Zone failures
        - Synchronous with all 3 Zones
        - 12.9s of durability
        - Available in 8 Regions
        - SLA: 99.9%
  3. *Geographical Redundant Storage (GRS)*
        - Data redundancy option is offered by Azure of the entire region experiences a power failure
        - So, data copies are kept at two or more sites spread across various Geographic areas
        - If the primary site is unavailable, accessing data from the secondary location requires a Geo-failover
        - Multiple replicas across each of 2 regions
        - Protects against major regional disasters
        - Asynchronous to secondary
        - 16.9s of durability
        - SLA: 99.9%
  4. *Read-Access Geographical Redundant Storage (RA-GRS)*
        - When the primary region goes down, this data redundancy option makes sure that the secondary region is still accessible
        - Basically, it is a backup of GRS
        - GRS + Read-Access to secondary
        - Separate secondary endpoint
        - RPO delay to secondary can be queried
        - SLA: 99.99% (read), 99.9% (write)

Q3. **How CI/CD is achieved in the case of Azure Databricks?**
