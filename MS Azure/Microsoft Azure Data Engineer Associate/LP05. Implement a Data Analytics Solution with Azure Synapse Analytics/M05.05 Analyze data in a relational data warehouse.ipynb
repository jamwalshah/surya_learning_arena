{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# M05.05 Analyze data in a relational data warehouse\n",
        "\n"
      ],
      "metadata": {
        "id": "HXb_EdifTL8B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unit 1 of 8\n",
        "\n"
      ],
      "metadata": {
        "id": "iPkkQM8s9G2m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Introduction\n",
        "\n",
        "Relational data warehouses are at the center of most enterprise business intelligence (BI) solutions. While the specific details may vary across data warehouse implementations, a common pattern based on a denormalized, multidimensional schema has emerged as the standard design for a relational data warehouse.\n",
        "\n",
        "Azure Synapse Analytics includes a highly scalable relational database engine that is optimized for data warehousing workloads. By using *dedicated SQL pools* in Azure Synapse Analytics, you can create databases that are capable of hosting and querying huge volumes of data in relational tables.\n",
        "\n",
        "In this module, you'll learn how to:\n",
        "\n",
        "- Design a schema for a relational data warehouse.\n",
        "- Create fact, dimension, and staging tables.\n",
        "- Use SQL to load data into data warehouse tables.\n",
        "- Use SQL to query relational data warehouse tables.\n",
        "\n"
      ],
      "metadata": {
        "id": "RuCMxxco9GzI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Next unit: Design a data warehouse schema\n",
        "\n"
      ],
      "metadata": {
        "id": "ZKCLtBO59Gws"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unit 2 of 8\n",
        "\n"
      ],
      "metadata": {
        "id": "y7sRlw9T9GuX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Design a data warehouse schema\n",
        "\n",
        "Like all relational databases, a data warehouse contains tables in which the data you want to analyze is stored. Most commonly, these tables are organized in a schema that is optimized for multidimensional modeling, in which numerical measures associated with events known as *facts* can be aggregated by the attributes of associated entities across multiple *dimensions*. For example, measures associated with a sales order (such as the amount paid or the quantity of items ordered) can be aggregated by attributes of the date on which the sale occurred, the customer, the store, and so on.\n",
        "\n"
      ],
      "metadata": {
        "id": "ShiEiSmq9Grz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Tables in a data warehouse\n",
        "\n",
        "A common pattern for relational data warehouses is to define a schema that includes two kinds of table: *dimension* tables and *fact* tables.\n",
        "\n"
      ],
      "metadata": {
        "id": "NlES5TeY9Gpb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Dimension tables\n",
        "\n",
        "Dimension tables describe business entities, such as products, people, places, and dates. Dimension tables contain columns for attributes of an entity. For example, a customer entity might have a first name, a last name, an email address, and a postal address (which might consist of a street address, a city, a postal code, and a country or region). In addition to attribute columns, a dimension table contains a unique key column that uniquely identifies each row in the table. In fact, it's common for a dimension table to include ***two*** key columns:\n",
        "\n",
        "- a *surrogate* key that is specific to the data warehouse and uniquely identifies each row in the dimension table in the data warehouse - usually an incrementing integer number.\n",
        "- An *alternate* key, often a *natural* or *business* key that is used to identify a specific instance of an entity in the transactional source system from which the entity record originated - such as a product code or a customer ID.\n",
        "\n",
        "> **Note:** Why have two keys? There are a few good reasons:\n",
        ">\n",
        "> - The data warehouse may be populated with data from multiple source systems, which can lead to the risk of duplicate or incompatible business keys.\n",
        "> - Simple numeric keys generally perform better in queries that join lots of tables - a common pattern in data warehouses.\n",
        "> - Attributes of entities may change over time - for example, a customer might change their address. Since the data warehouse is used to support historic reporting, you may want to retain a record for each instance of an entity at multiple points in time; so that, for example, sales orders for a specific customer are counted for the city where they lived at the time the order was placed. In this case, multiple customer records would have the same business key associated with the customer, but different surrogate keys for each discrete address where the customer lived at various times.\n",
        "\n",
        "An example of a dimension table for customer might contain the following data:\n",
        "\n",
        "| CustomerKey | CustomerAltKey | Name | Email | Street | City | PostalCode | CountryRegion |\n",
        "| :-- | :-- | :-- | :-- | :-- | :-- | :-- | :-- |\n",
        "| 123 | I-543 | Navin Jones | [navin1@contoso.com](navin1@contoso.com) | 1 Main St. | Seattle | 90000 | United States |\n",
        "| 124 | R-589 | Mary Smith | [mary2@contoso.com](mary2@contoso.com) | 234 190th Ave | Buffalo | 50001 | United States |\n",
        "| 125 | I-321 | Antoine Dubois | [antoine1@contoso.com](antoine1@contoso.com) | 2 Rue Jolie | Paris | 20098 | France |\n",
        "| 126 | I-543 | Navin Jones | [navin1@contoso.com](navin1@contoso.com) | 24 125th Ave. | New York | 50000 | United States |\n",
        "| ... | ... | ... | ... | ... | ... | ... | ... |\n",
        "\n",
        "> **Note:** Observe that the table contains two records for *Navin Jones*. Both records use the same alternate key to identify this person (*I-543*), but each record has a different surrogate key. From this, you can surmise that the customer moved from Seattle to New York. Sales made to the customer while living in Seattle are associated with the key *123*, while purchases made after moving to New York are recorded against record *126*.\n",
        "\n",
        "In addition to dimension tables that represent business entities, it's common for a data warehouse to include a dimension table that represents *time*. This table enables data analysts to aggregate data over temporal intervals. Depending on the type of data you need to analyze, the lowest granularity (referred to as the *grain*) of a time dimension could represent times (to the hour, second, millisecond, nanosecond, or even lower), or dates.\n",
        "\n",
        "An example of a time dimension table with a grain at the date level might contain the following data:\n",
        "\n",
        "| DateKey | DateAltKey | DayOfWeek | DayOfMonth | Weekday | Month | MonthName | Quarter | Year |\n",
        "| :-- | :-- | :-- | :-- | :-- | :-- | :-- | :-- | :-- |\n",
        "| 19990101 | 01-01-1999 | 6 | 1 | Friday | 1 | January | 1 | 1999 |\n",
        "| ... | ... | ... | ... | ... | ... | ... | ... | ... |\n",
        "| 20220101 | 01-01-2022 | 7 | 1 | Saturday | 1 | January | 1 | 2022 |\n",
        "| 20220102 | 02-01-2022 | 1 | 2 | Sunday | 1 | January | 1 | 2022 |\n",
        "| ... | ... | ... | ... | ... | ... | ... | ... | ... |\n",
        "| 20301231 | 31-12-2030 | 3 | 31 | Tuesday | 12 | December | 4 | 2030 |\n",
        "\n",
        "The timespan covered by the records in the table must include the earliest and latest points in time for any associated events recorded in a related fact table. Usually there's a record for every interval at the appropriate grain in between.\n",
        "\n"
      ],
      "metadata": {
        "id": "xW1zDUSN9GnH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Fact tables\n",
        "\n",
        "Fact tables store details of observations or events; for example, sales orders, stock balances, exchange rates, or recorded temperatures. A fact table contains columns for numeric values that can be aggregated by dimensions. In addition to the numeric columns, a fact table contains key columns that reference unique keys in related dimension tables.\n",
        "\n",
        "For example, a fact table containing details of sales orders might contain the following data:\n",
        "\n",
        "| OrderDateKey | CustomerKey | StoreKey | ProductKey | OrderNo | LineItemNo | Quantity | UnitPrice | Tax | ItemTotal |\n",
        "| :-- | :-- | :-- | :-- | :-- | :-- | :-- | :-- | :-- | :-- |\n",
        "| 20220101 | 123 | 5 | 701 | 1001 | 1 | 2 | 2.50 | 0.50 | 5.50 |\n",
        "| 20220101 | 123 | 5 | 765 | 1001 | 2 | 1 | 2.00 | 0.20 | 2.20 |\n",
        "| 20220102 | 125 | 2 | 723 | 1002 | 1 | 1 | 4.99 | 0.49 | 5.48 |\n",
        "| 20220103 | 126 | 1 | 823 | 1003 | 1 | 1 | 7.99 | 0.80 | 8.79 |\n",
        "| ... | ... | ... | ... | ... | ... | ... | ... | ... | ... |\n",
        "\n",
        "A fact table's dimension key columns determine its grain. For example, the sales orders fact table includes keys for dates, customers, stores, and products. An order might include multiple products, so the grain represents line items for individual products sold in stores to customers on specific days.\n",
        "\n"
      ],
      "metadata": {
        "id": "7QTrTsfR9Gk1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Data warehouse schema designs\n",
        "\n",
        "In most transactional databases that are used in business applications, the data is *normalized* to reduce duplication. In a data warehouse however, the dimension data is generally *de-normalized* to reduce the number of joins required to query the data.\n",
        "\n",
        "Often, a data warehouse is organized as a *star* schema, in which a fact table is directly related to the dimension tables, as shown in this example:\n",
        "\n",
        "![A diagram showing a star schema.](https://learn.microsoft.com/en-us/training/wwl-data-ai/design-multidimensional-schema-to-optimize-analytical-workloads/media/star-schema.png)\n",
        "\n",
        "The attributes of an entity can be used to aggregate measures in fact tables over multiple hierarchical levels - for example, to find total sales revenue by country or region, city, postal code, or individual customer. The attributes for each level can be stored in the same dimension table. However, when an entity has a large number of hierarchical attribute levels, or when some attributes can be shared by multiple dimensions (for example, both customers and stores have a geographical address), it can make sense to apply some normalization to the dimension tables and create a *snowflake* schema, as shown in the following example:\n",
        "\n",
        "![A diagram showing a snowflake schema.](https://learn.microsoft.com/en-us/training/wwl-data-ai/design-multidimensional-schema-to-optimize-analytical-workloads/media/snowflake-schema.png)\n",
        "\n",
        "In this case, the **DimProduct** table has been normalized to create separate dimension tables for product categories and suppliers, and a **DimGeography** table has been added to represent geographical attributes for both customers and stores. Each row in the DimProduct table contains key values for the corresponding rows in the **DimCategory** and **DimSupplier** tables; and each row in the **DimCustomer** and **DimStore** tables contains a key value for the corresponding row in the **DimGeography** table.\n",
        "\n"
      ],
      "metadata": {
        "id": "Lk2qVaW39Giq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Next unit: Create data warehouse tables\n",
        "\n"
      ],
      "metadata": {
        "id": "2ON0rrIg9GgU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unit 3 of 8\n",
        "\n"
      ],
      "metadata": {
        "id": "O7swuvm09Gd0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create data warehouse tables\n",
        "\n",
        "Now that you understand the basic architectural principles for a relational data warehouse schema, let's explore how to create a data warehouse.\n",
        "\n"
      ],
      "metadata": {
        "id": "WSYDJMOR9GbX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Creating a dedicated SQL pool\n",
        "\n",
        "To create a relational data warehouse in Azure Synapse Analytics, you must create a dedicated SQL Pool. The simplest way to do this in an existing Azure Synapse Analytics workspace is to use the **Manage** page in Azure Synapse Studio, as shown here:\n",
        "\n",
        "![A screenshot of the SQL pools tab in the Manage page of Synapse Studio.](https://learn.microsoft.com/en-us/training/wwl-data-ai/design-multidimensional-schema-to-optimize-analytical-workloads/media/sql-pools.png)\n",
        "\n",
        "When provisioning a dedicated SQL pool, you can specify the following configuration settings:\n",
        "\n",
        "- A unique name for the dedicated SQL pool.\n",
        "- A performance level for the SQL pool, which can range from *DW100c* to *DW30000c* and which determines the cost per hour for the pool when it's running.\n",
        "- Whether to start with an empty pool or restore an existing database from a backup.\n",
        "- The *collation* of the SQL pool, which determines sort order and string comparison rules for the database. (*You can't change the collation after creation*).\n",
        "\n",
        "After creating a dedicated SQL pool, you can control its running state in the **Manage** page of Synapse Studio; pausing it when not required to prevent unnecessary costs.\n",
        "\n",
        "When the pool is running, you can explore it on the **Data** page, and create SQL scripts to run in it.\n",
        "\n"
      ],
      "metadata": {
        "id": "8lXQiHU99GY_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Considerations for creating tables\n",
        "\n",
        "To create tables in the dedicated SQL pool, you use the `CREATE TABLE` (or sometimes the `CREATE EXTERNAL TABLE`) Transact-SQL statement. The specific options used in the statement depend on the type of table you're creating, which can include:\n",
        "\n",
        "- Fact tables\n",
        "- Dimension tables\n",
        "- Staging tables\n",
        "\n",
        "> **Note:** The data warehouse is composed of *fact* and *dimension* tables as discussed previously. *Staging* tables are often used as part of the data warehousing loading process to ingest data from source systems.\n",
        "\n",
        "When designing a star schema model for small or medium sized datasets you can use your preferred database, such as Azure SQL. For larger data sets you may benefit from implementing your data warehouse in Azure Synapse Analytics instead of SQL Server. It's important to understand some key differences when creating tables in Synapse Analytics.\n",
        "\n"
      ],
      "metadata": {
        "id": "NP-0oPOG9GWu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Data integrity constraints\n",
        "\n",
        "Dedicated SQL pools in Synapse Analytics don't support *foreign key* and *unique* constraints as found in other relational database systems like SQL Server. This means that jobs used to load data must maintain uniqueness and referential integrity for keys, without relying on the table definitions in the database to do so.\n",
        "\n",
        "> **Tip:** For more information about constraints in Azure Synapse Analytics dedicated SQL pools, see [Primary key, foreign key, and unique key using dedicated SQL pool in Azure Synapse Analytics](https://learn.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-table-constraints).\n",
        "\n"
      ],
      "metadata": {
        "id": "qrdZc6ck9GUP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Indexes\n",
        "\n",
        "While Synapse Analytics dedicated SQL pools support *clustered* indexes as found in SQL Server, the default index type is *clustered columnstore*. This index type offers a significant performance advantage when querying large quantities of data in a typical data warehouse schema and should be used where possible. However, some tables may include data types that can't be included in a clustered columnstore index (for example, VARBINARY(MAX)), in which case a clustered index can be used instead.\n",
        "\n",
        "> **Tip:** For more information about indexing in Azure Synapse Analytics dedicated SQL pools, see [Indexes on dedicated SQL pool tables in Azure Synapse Analytics](https://learn.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-tables-index).\n",
        "\n"
      ],
      "metadata": {
        "id": "IeKYoHwI9GR8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Distribution\n",
        "\n",
        "Azure Synapse Analytics dedicated SQL pools use a [massively parallel processing (MPP) architecture](https://learn.microsoft.com/en-us/azure/architecture/data-guide/relational-data/data-warehousing#data-warehousing-in-azure), as opposed to the symmetric multiprocessing (SMP) architecture used in most OLTP database systems. In an MPP system, the data in a table is distributed for processing across a pool of nodes. Synapse Analytics supports the following kinds of distribution:\n",
        "\n",
        "- **Hash**: A deterministic hash value is calculated for the specified column and used to assign the row to a compute node.\n",
        "- **Round-robin**: Rows are distributed evenly across all compute nodes.\n",
        "- **Replicated**: A copy of the table is stored on each compute node.\n",
        "\n",
        "The table type often determines which option to choose for distributing the table.\n",
        "\n",
        "| Table type | Recommended distribution option |\n",
        "| :-- | :-- |\n",
        "| Dimension | Use replicated distribution for smaller tables to avoid data shuffling when joining to distributed fact tables. If tables are too large to store on each compute node, use hash distribution. |\n",
        "| Fact | Use hash distribution with clustered columnstore index to distribute fact tables across compute nodes. |\n",
        "| Staging | Use round-robin distribution for staging tables to evenly distribute data across compute nodes. |\n",
        "\n",
        "> **Tip:** For more information about distribution strategies for tables in Azure Synapse Analytics, see [Guidance for designing distributed tables using dedicated SQL pool in Azure Synapse Analytics](https://learn.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-tables-distribute).\n",
        "\n"
      ],
      "metadata": {
        "id": "lnvjkYef9GPf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Creating dimension tables\n",
        "\n",
        "When you create a dimension table, ensure that the table definition includes surrogate and alternate keys as well as columns for the attributes of the dimension that you want to use to group aggregations. It's often easiest to use an `IDENTITY` column to auto-generate an incrementing surrogate key (otherwise you need to generate unique keys every time you load data). The following example shows a `CREATE TABLE` statement for a hypothetical **DimCustomer** dimension table.\n",
        "\n",
        "```sql\n",
        "CREATE TABLE dbo.DimCustomer\n",
        "(\n",
        "    CustomerKey INT IDENTITY NOT NULL,\n",
        "    CustomerAlternateKey NVARCHAR(15) NULL,\n",
        "    CustomerName NVARCHAR(80) NOT NULL,\n",
        "    EmailAddress NVARCHAR(50) NULL,\n",
        "    Phone NVARCHAR(25) NULL,\n",
        "    StreetAddress NVARCHAR(100),\n",
        "    City NVARCHAR(20),\n",
        "    PostalCode NVARCHAR(10),\n",
        "    CountryRegion NVARCHAR(20)\n",
        ")\n",
        "WITH\n",
        "(\n",
        "    DISTRIBUTION = REPLICATE,\n",
        "    CLUSTERED COLUMNSTORE INDEX\n",
        ");\n",
        "```\n",
        "\n",
        "> **Note:** If desired, you can create a specific *schema* as a namespace for your tables. In this example, the default **dbo** schema is used.\n",
        "\n",
        "If you intend to use a *snowflake* schema in which dimension tables are related to one another, you should include the key for the *parent* dimension in the definition of the **child** dimension table. For example, the following SQL code could be used to move the geographical address details from the **DimCustomer** table to a separate **DimGeography** dimension table:\n",
        "\n",
        "```sql\n",
        "CREATE TABLE dbo.DimGeography\n",
        "(\n",
        "    GeographyKey INT IDENTITY NOT NULL,\n",
        "    GeographyAlternateKey NVARCHAR(10) NULL,\n",
        "    StreetAddress NVARCHAR(100),\n",
        "    City NVARCHAR(20),\n",
        "    PostalCode NVARCHAR(10),\n",
        "    CountryRegion NVARCHAR(20)\n",
        ")\n",
        "WITH\n",
        "(\n",
        "    DISTRIBUTION = REPLICATE,\n",
        "    CLUSTERED COLUMNSTORE INDEX\n",
        ");\n",
        "\n",
        "CREATE TABLE dbo.DimCustomer\n",
        "(\n",
        "    CustomerKey INT IDENTITY NOT NULL,\n",
        "    CustomerAlternateKey NVARCHAR(15) NULL,\n",
        "    GeographyKey INT NULL,\n",
        "    CustomerName NVARCHAR(80) NOT NULL,\n",
        "    EmailAddress NVARCHAR(50) NULL,\n",
        "    Phone NVARCHAR(25) NULL\n",
        ")\n",
        "WITH\n",
        "(\n",
        "    DISTRIBUTION = REPLICATE,\n",
        "    CLUSTERED COLUMNSTORE INDEX\n",
        ");\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "joy2bGBx9GMp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Time dimension tables\n",
        "\n",
        "Most data warehouses include a *time* dimension table that enables you to aggregate data by multiple hierarchical levels of time interval. For example, the following example creates a **DimDate** table with attributes that relate to specific dates.\n",
        "\n",
        "```sql\n",
        "CREATE TABLE dbo.DimDate\n",
        "(\n",
        "    DateKey INT NOT NULL,\n",
        "    DateAltKey DATETIME NOT NULL,\n",
        "    DayOfMonth INT NOT NULL,\n",
        "    DayOfWeek INT NOT NULL,\n",
        "    DayName NVARCHAR(15) NOT NULL,\n",
        "    MonthOfYear INT NOT NULL,\n",
        "    MonthName NVARCHAR(15) NOT NULL,\n",
        "    CalendarQuarter INT  NOT NULL,\n",
        "    CalendarYear INT NOT NULL,\n",
        "    FiscalQuarter INT NOT NULL,\n",
        "    FiscalYear INT NOT NULL\n",
        ")\n",
        "WITH\n",
        "(\n",
        "    DISTRIBUTION = REPLICATE,\n",
        "    CLUSTERED COLUMNSTORE INDEX\n",
        ");\n",
        "```\n",
        "\n",
        "> **Tip:** A common pattern when creating a dimension table for dates is to use the numeric date in *DDMMYYYY* or *YYYYMMDD* format as an integer surrogate key, and the date as a `DATE` or `DATETIME` datatype as the alternate key.\n",
        "\n"
      ],
      "metadata": {
        "id": "BjIC1qnu9GKT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Creating fact tables\n",
        "\n",
        "Fact tables include the keys for each dimension to which they're related, and the attributes and numeric measures for specific events or observations that you want to analyze.\n",
        "\n",
        "The following code example creates a hypothetical fact table named **FactSales** that is related to multiple dimensions through key columns (date, customer, product, and store)\n",
        "\n",
        "```sql\n",
        "CREATE TABLE dbo.FactSales\n",
        "(\n",
        "    OrderDateKey INT NOT NULL,\n",
        "    CustomerKey INT NOT NULL,\n",
        "    ProductKey INT NOT NULL,\n",
        "    StoreKey INT NOT NULL,\n",
        "    OrderNumber NVARCHAR(10) NOT NULL,\n",
        "    OrderLineItem INT NOT NULL,\n",
        "    OrderQuantity SMALLINT NOT NULL,\n",
        "    UnitPrice DECIMAL NOT NULL,\n",
        "    Discount DECIMAL NOT NULL,\n",
        "    Tax DECIMAL NOT NULL,\n",
        "    SalesAmount DECIMAL NOT NULL\n",
        ")\n",
        "WITH\n",
        "(\n",
        "    DISTRIBUTION = HASH(OrderNumber),\n",
        "    CLUSTERED COLUMNSTORE INDEX\n",
        ");\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "RIsFWZpMTL4p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Creating staging tables\n",
        "\n",
        "Staging tables are used as temporary storage for data as it's being loaded into the data warehouse. A typical pattern is to structure the table to make it as efficient as possible to ingest the data from its external source (often files in a data lake) into the relational database, and then use SQL statements to load the data from the staging tables into the dimension and fact tables.\n",
        "\n",
        "The following code example creates a staging table for product data that will ultimately be loaded into a dimension table:\n",
        "\n",
        "```sql\n",
        "CREATE TABLE dbo.StageProduct\n",
        "(\n",
        "    ProductID NVARCHAR(10) NOT NULL,\n",
        "    ProductName NVARCHAR(200) NOT NULL,\n",
        "    ProductCategory NVARCHAR(200) NOT NULL,\n",
        "    Color NVARCHAR(10),\n",
        "    Size NVARCHAR(10),\n",
        "    ListPrice DECIMAL NOT NULL,\n",
        "    Discontinued BIT NOT NULL\n",
        ")\n",
        "WITH\n",
        "(\n",
        "    DISTRIBUTION = ROUND_ROBIN,\n",
        "    CLUSTERED COLUMNSTORE INDEX\n",
        ");\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "4rnaE1ZJTL1a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Using external tables\n",
        "\n",
        "In some cases, if the data to be loaded is in files with an appropriate structure, it can be more effective to create external tables that reference the file location. This way, the data can be read directly from the source files instead of being loaded into the relational store. The following example, shows how to create an external table that references files in the data lake associated with the Synapse workspace:\n",
        "\n",
        "```sql\n",
        "-- External data source links to data lake location\n",
        "CREATE EXTERNAL DATA SOURCE StagedFiles\n",
        "WITH (\n",
        "    LOCATION = 'https://mydatalake.blob.core.windows.net/data/stagedfiles/'\n",
        ");\n",
        "GO\n",
        "\n",
        "-- External format specifies file format\n",
        "CREATE EXTERNAL FILE FORMAT ParquetFormat\n",
        "WITH (\n",
        "    FORMAT_TYPE = PARQUET,\n",
        "    DATA_COMPRESSION = 'org.apache.hadoop.io.compress.SnappyCodec'\n",
        ");\n",
        "GO\n",
        "\n",
        "-- External table references files in external data source\n",
        "CREATE EXTERNAL TABLE dbo.ExternalStageProduct\n",
        "(\n",
        "    ProductID NVARCHAR(10) NOT NULL,\n",
        "    ProductName NVARCHAR(200) NOT NULL,\n",
        "    ProductCategory NVARCHAR(200) NOT NULL,\n",
        "    Color NVARCHAR(10),\n",
        "    Size NVARCHAR(10),\n",
        "    ListPrice DECIMAL NOT NULL,\n",
        "    Discontinued BIT NOT NULL\n",
        ")\n",
        "WITH\n",
        "(\n",
        "    DATA_SOURCE = StagedFiles,\n",
        "    LOCATION = 'products/*.parquet',\n",
        "    FILE_FORMAT = ParquetFormat\n",
        ");\n",
        "GO\n",
        "```\n",
        "\n",
        "> **Note:** For more information about using external tables, see [Use external tables with Synapse SQL](https://learn.microsoft.com/en-us/azure/synapse-analytics/sql/develop-tables-external-tables) in the Azure Synapse Analytics documentation.\n",
        "\n"
      ],
      "metadata": {
        "id": "0VWRzFsUTLzJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Next unit: Load data warehouse tables\n",
        "\n"
      ],
      "metadata": {
        "id": "-GwnOi9qTLv7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unit 4 of 8\n",
        "\n"
      ],
      "metadata": {
        "id": "VDVfo-UnTLss"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load data warehouse tables\n",
        "\n",
        "At a basic level, loading a data warehouse is typically achieved by adding new data from files in a data lake into tables in the data warehouse. The `COPY` statement is an effective way to accomplish this task, as shown in the following example:\n",
        "\n",
        "```sql\n",
        "COPY INTO dbo.StageProducts\n",
        "    (ProductID, ProductName, ProductCategory, Color, Size, ListPrice, Discontinued)\n",
        "FROM 'https://mydatalake.blob.core.windows.net/data/stagedfiles/products/*.parquet'\n",
        "WITH\n",
        "(\n",
        "    FILE_TYPE = 'PARQUET',\n",
        "    MAXERRORS = 0,\n",
        "    IDENTITY_INSERT = 'OFF'\n",
        ");\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "zNOqBN1iTLpg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Considerations for designing a data warehouse load process\n",
        "\n",
        "One of the most common patterns for loading a data warehouse is to transfer data from source systems to files in a data lake, ingest the file data into staging tables, and then use SQL statements to load the data from the staging tables into the dimension and fact tables. Usually data loading is performed as a periodic batch process in which inserts and updates to the data warehouse are coordinated to occur at a regular interval (for example, daily, weekly, or monthly).\n",
        "\n",
        "In most cases, you should implement a data warehouse load process that performs tasks in the following order:\n",
        "\n",
        "1. Ingest the new data to be loaded into a data lake, applying pre-load cleansing or transformations as required.\n",
        "2. Load the data from files into staging tables in the relational data warehouse.\n",
        "3. Load the dimension tables from the dimension data in the staging tables, updating existing rows or inserting new rows and generating surrogate key values as necessary.\n",
        "4. Load the fact tables from the fact data in the staging tables, looking up the appropriate surrogate keys for related dimensions.\n",
        "5. Perform post-load optimization by updating indexes and table distribution statistics.\n",
        "\n",
        "After using the `COPY` statement to load data into staging tables, you can use a combination of `INSERT`, `UPDATE`, `MERGE`, and `CREATE TABLE AS SELECT` (*CTAS*) statements to load the staged data into dimension and fact tables.\n",
        "\n",
        "> **Note:** Implementing an effective data warehouse loading solution requires careful consideration of how to manage surrogate keys, slowly changing dimensions, and other complexities inherent in a relational data warehouse schema. To learn more about techniques for loading a data warehouse, consider completing the [Load data into a relational data warehouse](https://learn.microsoft.com/en-us/training/modules/load-optimize-data-into-relational-data-warehouse) module.\n",
        "\n"
      ],
      "metadata": {
        "id": "RRjeXKoJTLmg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Next unit: Query a data warehouse\n",
        "\n"
      ],
      "metadata": {
        "id": "W25OdVy7TLb5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unit 5 of 8\n",
        "\n"
      ],
      "metadata": {
        "id": "0rAHXP6FTLYZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Query a data warehouse\n",
        "\n",
        "When the dimension and fact tables in a data warehouse have been loaded with data, you can use SQL to query the tables and analyze the data they contain. The Transact-SQL syntax used to query tables in a Synapse dedicated SQL pool is similar to SQL used in SQL Server or Azure SQL Database.\n",
        "\n"
      ],
      "metadata": {
        "id": "jJIuZCwJTLVR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Aggregating measures by dimension attributes\n",
        "\n",
        "Most data analytics with a data warehouse involves aggregating numeric measures in fact tables by attributes in dimension tables. Because of the way a star or snowflake schema is implemented, queries to perform this kind of aggregation rely on `JOIN` clauses to connect fact tables to dimension tables, and a combination of aggregate functions and `GROUP BY` clauses to define the aggregation hierarchies.\n",
        "\n",
        "For example, the following SQL queries the **FactSales** and **DimDate** tables in a hypothetical data warehouse to aggregate sales amounts by year and quarter:\n",
        "\n",
        "```sql\n",
        "SELECT  dates.CalendarYear,\n",
        "        dates.CalendarQuarter,\n",
        "        SUM(sales.SalesAmount) AS TotalSales\n",
        "FROM dbo.FactSales AS sales\n",
        "JOIN dbo.DimDate AS dates ON sales.OrderDateKey = dates.DateKey\n",
        "GROUP BY dates.CalendarYear, dates.CalendarQuarter\n",
        "ORDER BY dates.CalendarYear, dates.CalendarQuarter;\n",
        "```\n",
        "\n",
        "The results from this query would look similar to the following table:\n",
        "\n",
        "| CalendarYear | CalendarQuarter | TotalSales |\n",
        "| :-- | :-- | :-- |\n",
        "| 2020 | 1 | 25980.16 |\n",
        "| 2020 | 2 | 27453.87 |\n",
        "| 2020 | 3 | 28527.15 |\n",
        "| 2020 | 4 | 31083.45 |\n",
        "| 2021 | 1 | 34562.96 |\n",
        "| 2021 | 2 | 36162.27 |\n",
        "| ... | ... | ... |\n",
        "\n",
        "You can join as many dimension tables as needed to calculate the aggregations you need. For example, the following code extends the previous example to break down the quarterly sales totals by city based on the customer's address details in the **DimCustomer** table:\n",
        "\n",
        "```sql\n",
        "SELECT  dates.CalendarYear,\n",
        "        dates.CalendarQuarter,\n",
        "        custs.City,\n",
        "        SUM(sales.SalesAmount) AS TotalSales\n",
        "FROM dbo.FactSales AS sales\n",
        "JOIN dbo.DimDate AS dates ON sales.OrderDateKey = dates.DateKey\n",
        "JOIN dbo.DimCustomer AS custs ON sales.CustomerKey = custs.CustomerKey\n",
        "GROUP BY dates.CalendarYear, dates.CalendarQuarter, custs.City\n",
        "ORDER BY dates.CalendarYear, dates.CalendarQuarter, custs.City;\n",
        "```\n",
        "\n",
        "This time, the results include a quarterly sales total for each city:\n",
        "\n",
        "| CalendarYear | CalendarQuarter | City | TotalSales |\n",
        "| :-- | :-- | :-- | :-- |\n",
        "| 2020 | 1 | Amsterdam | 5982.53 |\n",
        "| 2020 | 1 | Berlin | 2826.98 |\n",
        "| 2020 | 1 | Chicago | 5372.72 |\n",
        "| ... | ... | ... | .. |\n",
        "| 2020 | 2 | Amsterdam | 7163.93 |\n",
        "| 2020 | 2 | Berlin | 8191.12 |\n",
        "| 2020 | 2 | Chicago | 2428.72 |\n",
        "| ... | ... | ... | .. |\n",
        "| 2020 | 3 | Amsterdam | 7261.92 |\n",
        "| 2020 | 3 | Berlin | 4202.65 |\n",
        "| 2020 | 3 | Chicago | 2287.87 |\n",
        "| ... | ... | ... | .. |\n",
        "| 2020 | 4 | Amsterdam | 8262.73 |\n",
        "| 2020 | 4 | Berlin | 5373.61 |\n",
        "| 2020 | 4 | Chicago | 7726.23 |\n",
        "| ... | ... | ... | .. |\n",
        "| 2021 | 1 | Amsterdam | 7261.28 |\n",
        "| 2021 | 1 | Berlin | 3648.28 |\n",
        "| 2021 | 1 | Chicago | 1027.27 |\n",
        "| ... | ... | ... | .. |\n",
        "\n"
      ],
      "metadata": {
        "id": "KlUqYoObTLSO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Joins in a snowflake schema\n",
        "\n",
        "When using a snowflake schema, dimensions may be partially normalized; requiring multiple joins to relate fact tables to snowflake dimensions. For example, suppose your data warehouse includes a **DimProduct** dimension table from which the product categories have been normalized into a separate **DimCategory** table. A query to aggregate items sold by product category might look similar to the following example:\n",
        "\n",
        "```sql\n",
        "SELECT  cat.ProductCategory,\n",
        "        SUM(sales.OrderQuantity) AS ItemsSold\n",
        "FROM dbo.FactSales AS sales\n",
        "JOIN dbo.DimProduct AS prod ON sales.ProductKey = prod.ProductKey\n",
        "JOIN dbo.DimCategory AS cat ON prod.CategoryKey = cat.CategoryKey\n",
        "GROUP BY cat.ProductCategory\n",
        "ORDER BY cat.ProductCategory;\n",
        "```\n",
        "\n",
        "The results from this query include the number of items sold for each product category:\n",
        "\n",
        "| ProductCategory | ItemsSold |\n",
        "| :-- | :-- |\n",
        "| Accessories | 28271 |\n",
        "| Bits and pieces | 5368 |\n",
        "| ... | ... |\n",
        "\n",
        "> **Note:** JOIN clauses for **FactSales** and **DimProduct** and for **DimProduct** and **DimCategory** are both required, even though no fields from **DimProduct** are returned by the query.\n",
        "\n"
      ],
      "metadata": {
        "id": "Lidb2HrETLPc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Using ranking functions\n",
        "\n",
        "Another common kind of analytical query is to partition the results based on a dimension attribute and *rank* the results within each partition. For example, you might want to rank stores each year by their sales revenue. To accomplish this goal, you can use Transact-SQL *ranking* functions such as `ROW_NUMBER`, `RANK`, `DENSE_RANK`, and `NTILE`. These functions enable you to partition the data over categories, each returning a specific value that indicates the relative position of each row within the partition:\n",
        "\n",
        "- **ROW_NUMBER** returns the ordinal position of the row within the partition. For example, the first row is numbered 1, the second 2, and so on.\n",
        "- **RANK** returns the ranked position of each row in the ordered results. For example, in a partition of stores ordered by sales volume, the store with the highest sales volume is ranked 1. If multiple stores have the same sales volumes, they'll be ranked the same, and the rank assigned to subsequent stores reflects the number of stores that have higher sales volumes - including ties.\n",
        "- **DENSE_RANK** ranks rows in a partition the same way as **RANK**, but when multiple rows have the same rank, subsequent rows are ranking positions ignore ties.\n",
        "- **NTILE** returns the specified percentile in which the row falls. For example, in a partition of stores ordered by sales volume, `NTILE(4)` returns the quartile in which a store's sales volume places it.\n",
        "\n",
        "For example, consider the following query:\n",
        "\n",
        "```sql\n",
        "SELECT  ProductCategory,\n",
        "        ProductName,\n",
        "        ListPrice,\n",
        "        ROW_NUMBER() OVER\n",
        "            (PARTITION BY ProductCategory ORDER BY ListPrice DESC) AS RowNumber,\n",
        "        RANK() OVER\n",
        "            (PARTITION BY ProductCategory ORDER BY ListPrice DESC) AS Rank,\n",
        "        DENSE_RANK() OVER\n",
        "            (PARTITION BY ProductCategory ORDER BY ListPrice DESC) AS DenseRank,\n",
        "        NTILE(4) OVER\n",
        "            (PARTITION BY ProductCategory ORDER BY ListPrice DESC) AS Quartile\n",
        "FROM dbo.DimProduct\n",
        "ORDER BY ProductCategory;\n",
        "```\n",
        "\n",
        "The query partitions products into groupings based on their categories, and within each category partition, the relative position of each product is determined based on its list price. The results from this query might look similar to the following table:\n",
        "\n",
        "| ProductCategory | ProductName | ListPrice | RowNumber | Rank | DenseRank | Quartile |\n",
        "| :-- | :-- | :-- | :-- | :-- | :-- | :-- |\n",
        "| Accessories | Widget | 8.99 | 1 | 1 | 1 | 1 |\n",
        "| Accessories | Knicknak | 8.49 | 2 | 2 | 2 | 1 |\n",
        "| Accessories | Sprocket | 5.99 | 3 | 3 | 3 | 2 |\n",
        "| Accessories | Doodah | 5.99 | 4 | 3 | 3 | 2 |\n",
        "| Accessories | Spangle | 2.99 | 5 | 5 | 4 | 3 |\n",
        "| Accessories | Badabing | 0.25 | 6 | 6 | 5 | 4 |\n",
        "| Bits and pieces | Flimflam | 7.49 | 1 | 1 | 1 | 1 |\n",
        "| Bits and pieces | Snickity wotsit | 6.99 | 2 | 2 | 2 | 1 |\n",
        "| Bits and pieces | Flange | 4.25 | 3 | 3 | 3 | 2 |\n",
        "| ... | ... | ... | ... | ... | ... | ... |\n",
        "\n",
        "> **Note:** The sample results demonstrate the difference between `RANK` and `DENSE_RANK`. Note that in the *Accessories* category, the *Sprocket* and *Doodah* products have the same list price; and are both ranked as the 3rd highest priced product. The next highest priced product has a *RANK* of 5 (there are four products more expensive than it) and a *DENSE_RANK* of 4 (there are three higher prices).\n",
        ">\n",
        "> To learn more about ranking functions, see [Ranking Functions (Transact-SQL)](https://learn.microsoft.com/en-us/sql/t-sql/functions/ranking-functions-transact-sql) in the Azure Synapse Analytics documentation.\n",
        "\n"
      ],
      "metadata": {
        "id": "iQ1sEXi2TLMv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Retrieving an approximate count\n",
        "\n",
        "While the purpose of a data warehouse is primarily to support analytical data models and reports for the enterprise; data analysts and data scientists often need to perform some initial data exploration, just to determine the basic scale and distribution of the data.\n",
        "\n",
        "For example, the following query uses the `COUNT` function to retrieve the number of sales for each year in a hypothetical data warehouse:\n",
        "\n",
        "```sql\n",
        "SELECT dates.CalendarYear AS CalendarYear,\n",
        "    COUNT(DISTINCT sales.OrderNumber) AS Orders\n",
        "FROM FactSales AS sales\n",
        "JOIN DimDate AS dates ON sales.OrderDateKey = dates.DateKey\n",
        "GROUP BY dates.CalendarYear\n",
        "ORDER BY CalendarYear;\n",
        "```\n",
        "\n",
        "The results of this query might look similar to the following table:\n",
        "\n",
        "| CalendarYear | Orders |\n",
        "| :-- | :-- |\n",
        "| 2019 | 239870 |\n",
        "| 2020 | 284741 |\n",
        "| 2021 | 309272 |\n",
        "| ... | ... |\n",
        "\n",
        "The volume of data in a data warehouse can mean that even simple queries to count the number of records that meet specified criteria can take a considerable time to run. In many cases, a precise count isn't required - an approximate estimate will suffice. In such cases, you can use the `APPROX_COUNT_DISTINCT` function as shown in the following example:\n",
        "\n",
        "```sql\n",
        "SELECT dates.CalendarYear AS CalendarYear,\n",
        "    APPROX_COUNT_DISTINCT(sales.OrderNumber) AS ApproxOrders\n",
        "FROM FactSales AS sales\n",
        "JOIN DimDate AS dates ON sales.OrderDateKey = dates.DateKey\n",
        "GROUP BY dates.CalendarYear\n",
        "ORDER BY CalendarYear;\n",
        "```\n",
        "\n",
        "The `APPROX_COUNT_DISTINCT` function uses a *HyperLogLog* algorithm to retrieve an approximate count. The result is guaranteed to have a maximum error rate of 2% with 97% probability, so the results of this query with the same hypothetical data as before might look similar to the following table:\n",
        "\n",
        "| CalendarYear | ApproxOrders |\n",
        "| :-- | :-- |\n",
        "| 2019 | 235552 |\n",
        "| 2020 | 290436 |\n",
        "| 2021 | 304633 |\n",
        "| ... | ... |\n",
        "\n",
        "The counts are less accurate, but still sufficient for an approximate comparison of yearly sales. With a large volume of data, the query using the `APPROX_COUNT_DISTINCT` function completes more quickly, and the reduced accuracy may be an acceptable trade-off during basic data exploration.\n",
        "\n",
        "> **Note:** See the [APPROX_COUNT_DISTINCT](https://learn.microsoft.com/en-us/sql/t-sql/functions/approx-count-distinct-transact-sql) function documentation for more details.\n",
        "\n"
      ],
      "metadata": {
        "id": "tGind2isTLJs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Next unit: Exercise - Explore a data warehouse\n",
        "\n"
      ],
      "metadata": {
        "id": "DcrbeDyMTLGf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unit 6 of 8\n",
        "\n"
      ],
      "metadata": {
        "id": "2vBvKX1NTLDl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise - Explore a data warehouse\n",
        "\n",
        "Now it's your opportunity to explore a relational data warehouse. In this exercise, you'll use a provided script to provision an Azure Synapse Analytics workspace in your Azure subscription; and then explore a data warehouse that has been created for you.\n",
        "\n",
        "> **Note:** To complete this lab, you will need an [Azure subscription](https://aka.ms/mslearn-synapse-dw) in which you have administrative access.\n",
        "\n",
        "Launch the exercise and follow the instructions.\n",
        "\n",
        "[Launch Exercise](https://aka.ms/mslearn-synapse-dw)\n",
        "\n"
      ],
      "metadata": {
        "id": "T12SZalzTLAJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Next unit: Knowledge check\n",
        "\n"
      ],
      "metadata": {
        "id": "cgQFmKEGTK81"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unit 7 of 8\n",
        "\n"
      ],
      "metadata": {
        "id": "HR5zVln7TK5a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Knowledge check\n",
        "\n",
        "1. In which of the following table types should an insurance company store details of customer attributes by which claims will be aggregated?\n",
        "\n",
        "    - ☐ Staging table\n",
        "    - ☑ Dimension table\n",
        "    > Correct. Attributes of an entity by which numeric measures will be aggregated are stored in a dimension table.\n",
        "    - ☐ Fact table\n",
        "\n",
        "2. You create a dimension table for product data, assigning a unique numeric key for each row in a column named `ProductKey`. The `ProductKey` is only defined in the data warehouse. What kind of key is `ProductKey`?\n",
        "\n",
        "    - ☑ A surrogate key\n",
        "    > Correct. A surrogate key uniquely identifies each row in a dimension table, irrespective of keys used in source systems.\n",
        "    - ☐ An alternate key\n",
        "    - ☐ A business key\n",
        "\n",
        "3. What distribution option would be best for a sales fact table that will contain billions of records?\n",
        "\n",
        "    - ☑ `HASH`\n",
        "    > Correct. Hash distribution provides good read performance for a large table by distributing records across compute nodes based on the hash key.\n",
        "    - ☐ `ROUND_ROBIN`\n",
        "    - ☐ `REPLICATE`\n",
        "\n",
        "4. You need to write a query to return the total of the `UnitsProduced` numeric measure in the `FactProduction` table aggregated by the `ProductName` attribute in the `FactProduct` table. Both tables include a `ProductKey` surrogate key field. What should you do?\n",
        "\n",
        "    - ☐ Use two `SELECT` queries with a `UNION ALL` clause to combine the rows in the `FactProduction` table with those in the `FactProduct` table.\n",
        "    - ☐ Use a `SELECT` query against the `FactProduction` table with a `WHERE` clause to filter out rows with a `ProductKey` that doesn't exist in the `FactProduct` table.\n",
        "    - ☑ Use a `SELECT` query with a `SUM` function to total the `UnitsProduced` metric, using a `JOIN` on the `ProductKey` surrogate key to match the `FactProduction` records to the `FactProduct` records and a `GROUP BY` clause to aggregate by `ProductName`.\n",
        "    > Correct. To aggregate measures in a fact table by attributes in a dimension table, include an aggregate function for the measure, join the tables on the surrogate key, and group the results by the appropriate attributes.\n",
        "\n",
        "5. You use the `RANK` function in a query to rank customers in order of the number of purchases they have made. Five customers have made the same number of purchases and are all ranked equally as 1. What rank will the customer with the next highest number of purchases be assigned?\n",
        "\n",
        "    - ☐ Two\n",
        "    - ☑ Six\n",
        "    > Correct. There are five customers with a higher number of purchases, and `RANK` takes these into account.\n",
        "    - ☐ One\n",
        "\n",
        "6. You need to compare approximate production volumes by product while optimizing query response time. Which function should you use?\n",
        "\n",
        "    - ☐ `COUNT`\n",
        "    - ☐ `NTILE`\n",
        "    - ☑ `APPROX_COUNT_DISTINCT`\n",
        "    > Correct. `APPROX_COUNT_DISTINCT` returns an approximate count within 2% of the actual count while optimizing for minimal response time.\n",
        "\n"
      ],
      "metadata": {
        "id": "iLwngGUfTK14"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Next unit: Summary\n",
        "\n"
      ],
      "metadata": {
        "id": "d8cGoLpS-cXI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unit 8 of 8\n",
        "\n"
      ],
      "metadata": {
        "id": "NQzD1d1J-cTp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Summary\n",
        "\n",
        "Relational data warehousing skills are essential in multiple data professional roles, including data engineers, data analysts, and data scientists.\n",
        "\n",
        "In this module, you learned how to:\n",
        "\n",
        "- Design a schema for a relational data warehouse.\n",
        "- Create fact, dimension, and staging tables.\n",
        "- Use SQL to load data into data warehouse tables.\n",
        "- Use SQL to query relational data warehouse tables.\n",
        "\n"
      ],
      "metadata": {
        "id": "5a2PJSC5-cQ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Learn more\n",
        "\n",
        "To learn more about using Azure Synapse Analytics for relational data warehousing, refer to [Synapse POC playbook: Data warehousing with dedicated SQL pool in Azure Synapse Analytics](https://learn.microsoft.com/en-us/azure/synapse-analytics/guidance/proof-of-concept-playbook-dedicated-sql-pool).\n",
        "\n"
      ],
      "metadata": {
        "id": "gKK-NxxG-cOo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### All units complete\n",
        "\n"
      ],
      "metadata": {
        "id": "UYgf9IRH-cMg"
      }
    }
  ]
}