{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# M05.04 Use Delta Lake in Azure Synapse Analytics\n",
        "\n"
      ],
      "metadata": {
        "id": "tZ8qmkTt1fT2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unit 1 of 9\n",
        "\n"
      ],
      "metadata": {
        "id": "LWVF-W7CMHd6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Introduction\n",
        "\n",
        "Linux foundation *Delta Lake* is an open-source storage layer for Spark that enables relational database capabilities for batch and streaming data. By using Delta Lake, you can implement a *data lakehouse* architecture in Spark to support SQL_based data manipulation semantics with support for transactions and schema enforcement. The result is an analytical data store that offers many of the advantages of a relational database system with the flexibility of data file storage in a data lake.\n",
        "\n",
        "In this module, you'll learn how to:\n",
        "\n",
        "- Describe core features and capabilities of Delta Lake.\n",
        "- Create and use Delta Lake tables in a Synapse Analytics Spark pool.\n",
        "- Create Spark catalog tables for Delta Lake data.\n",
        "- Use Delta Lake tables for streaming data.\n",
        "- Query Delta Lake tables from a Synapse Analytics SQL pool.\n",
        "\n",
        "> **Note:** The version of Delta Lake available in an Azure Synapse Analytics pool depends on the version of Spark specified in the pool configuration. The information in this module reflects Delta Lake version 1.0, which is installed with Spark 3.1.\n",
        "\n"
      ],
      "metadata": {
        "id": "-INfFf0IMHbi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Next unit: Understand Delta Lake\n",
        "\n"
      ],
      "metadata": {
        "id": "eeraaadDMHSe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unit 2 of 9\n",
        "\n"
      ],
      "metadata": {
        "id": "OtQbrfNKMHQO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Understand Delta Lake\n",
        "\n",
        "Delta Lake is an open-source storage layer that adds relational database semantics to Spark-based data lake processing. Delta Lake is supported in Azure Synapse Analytics Spark pools for PySpark, Scala, and .NET code.\n",
        "\n",
        "The benefits of using Delta Lake in a Synapse Analytics Spark pool include:\n",
        "\n",
        "- **Relational tables that support querying and data modification**. With Delta Lake, you can store data in tables that support *CRUD* (create, read, update, and delete) operations. In other words, you can *select*, *insert*, *update*, and *delete* rows of data in the same way you would in a relational database system.\n",
        "- **Support for *ACID* transactions**. Relational databases are designed to support transactional data modifications that provide *atomicity* (transactions complete as a single unit of work), *consistency* (transactions leave the database in a consistent state), *isolation* (in-process transactions can't interfere with one another), and *durability* (when a transaction completes, the changes it made are persisted). Delta Lake brings this same transactional support to Spark by implementing a transaction log and enforcing serializable isolation for concurrent operations.\n",
        "- **Data versioning and *time travel***. Because all transactions are logged in the transaction log, you can track multiple versions of each table row and even use the *time travel* feature to retrieve a previous version of a row in a query.\n",
        "- **Support for batch and streaming data**. While most relational databases include tables that store static data, Spark includes native support for streaming data through the Spark Structured Streaming API. Delta Lake tables can be used as both *sinks* (destinations) and *sources* for streaming data.\n",
        "- **Standard formats and interoperability**. The underlying data for Delta Lake tables is stored in Parquet format, which is commonly used in data lake ingestion pipelines. Additionally, you can use the serverless SQL pool in Azure Synapse Analytics to query Delta Lake tables in SQL.\n",
        "\n",
        "> **Tip:** For more information about Delta Lake in Azure Synapse Analytics, see [What is Delta Lake](https://learn.microsoft.com/en-us/azure/synapse-analytics/spark/apache-spark-what-is-delta-lake) in the Azure Synapse Analytics documentation.\n",
        "\n"
      ],
      "metadata": {
        "id": "FifRfwTiMHNu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Next unit: Create Delta Lake tables\n",
        "\n"
      ],
      "metadata": {
        "id": "e7jeHO7iMHLV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unit 3 of 9\n",
        "\n"
      ],
      "metadata": {
        "id": "Bi6QMbamMHJA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create Delta Lake tables\n",
        "\n",
        "Delta lake is built on tables, which provide a relational storage abstraction over files in a data lake.\n",
        "\n"
      ],
      "metadata": {
        "id": "2b5mivZWMHGe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Creating a Delta Lake table from a dataframe\n",
        "\n",
        "One of the easiest ways to create a Delta Lake table is to save a dataframe in the *delta* format, specifying a path where the data files and related metadata information for the table should be stored.\n",
        "\n",
        "For example, the following PySpark code loads a dataframe with data from an existing file, and then saves that dataframe to a new folder location in *delta* format:\n",
        "\n",
        "```python\n",
        "# Load a file into a dataframe\n",
        "df = spark.read.load('/data/mydata.csv', format='csv', header=True)\n",
        "\n",
        "# Save the dataframe as a delta table\n",
        "delta_table_path = \"/delta/mydata\"\n",
        "df.write.format(\"delta\").save(delta_table_path)\n",
        "```\n",
        "\n",
        "After saving the delta table, the path location you specified includes parquet files for the data (regardless of the format of the source file you loaded into the dataframe) and a **_delta_log** folder containing the transaction log for the table.\n",
        "\n",
        "> **Note:** The transaction log records all data modifications to the table. By logging each modification, transactional consistency can be enforced and versioning information for the table can be retained.\n",
        "\n",
        "You can replace an existing Delta Lake table with the contents of a dataframe by using the **overwrite** mode, as shown here:\n",
        "\n",
        "```python\n",
        "new_df.write.format(\"delta\").mode(\"overwrite\").save(delta_table_path)\n",
        "```\n",
        "\n",
        "You can also add rows from a dataframe to an existing table by using the **append** mode:\n",
        "\n",
        "```python\n",
        "new_rows_df.write.format(\"delta\").mode(\"append\").save(delta_table_path)\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "n3JrJFt9MHEM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Making conditional updates\n",
        "\n",
        "While you can make data modifications in a dataframe and then replace a Delta Lake table by overwriting it, a more common pattern in a database is to insert, update or delete rows in an existing table as discrete transactional operations. To make such modifications to a Delta Lake table, you can use the **DeltaTable** object in the Delta Lake API, which supports *update*, *delete*, and *merge* operations. For example, you could use the following code to update the **price** column for all rows with a **category** column value of \"Accessories\":\n",
        "\n",
        "```python\n",
        "from delta.tables import *\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "# Create a deltaTable object\n",
        "deltaTable = DeltaTable.forPath(spark, delta_table_path)\n",
        "\n",
        "# Update the table (reduce price of accessories by 10%)\n",
        "deltaTable.update(\n",
        "    condition = \"Category == 'Accessories'\",\n",
        "    set = { \"Price\": \"Price * 0.9\" })\n",
        "```\n",
        "\n",
        "The data modifications are recorded in the transaction log, and new parquet files are created in the table folder as required.\n",
        "\n",
        "> **Tip:** For more information about using the Delta Lake API, see [the Delta Lake API documentation](https://docs.delta.io/latest/delta-apidoc.html).\n",
        "\n"
      ],
      "metadata": {
        "id": "NtQuGibxMHB3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Querying a previous version of a table\n",
        "\n",
        "Delta Lake tables support versioning through the transaction log. The transaction log records modifications made to the table, noting the timestamp and version number for each transaction. You can use this logged version data to view previous versions of the table - a feature known as *time travel*.\n",
        "\n",
        "You can retrieve data from a specific version of a Delta Lake table by reading the data from the delta table location into a dataframe, specifying the version required as a `versionAsOf` option:\n",
        "\n",
        "```python\n",
        "df = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(delta_table_path)\n",
        "```\n",
        "\n",
        "Alternatively, you can specify a timestamp by using the `timestampAsOf` option:\n",
        "\n",
        "```python\n",
        "df = spark.read.format(\"delta\").option(\"timestampAsOf\", '2022-01-01').load(delta_table_path)\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "qJdkWifDMG_g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Next unit: Create catalog tables\n",
        "\n"
      ],
      "metadata": {
        "id": "VoDiL6A1MG9E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unit 4 of 9\n",
        "\n"
      ],
      "metadata": {
        "id": "EwIv_DTZMG6v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create catalog tables\n",
        "\n",
        "So far we've considered Delta Lake table instances created from dataframes and modified through the Delta Lake API. You can also define Delta Lake tables as catalog tables in the Hive metastore for your Spark pool, and work with them using SQL.\n",
        "\n"
      ],
      "metadata": {
        "id": "wQY283U5MG4j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### *External* vs *managed* tables\n",
        "\n",
        "Tables in a Spark catalog, including Delta Lake tables, can be *managed* or *external*; and it's important to understand the distinction between these kinds of table.\n",
        "\n",
        "- A *managed* table is defined without a specified location, and the data files are stored within the storage used by the metastore. Dropping the table not only removes its metadata from the catalog, but also deletes the folder in which its data files are stored.\n",
        "- An *external* table is defined for a custom file location, where the data for the table is stored. The metadata for the table is defined in the Spark catalog. Dropping the table deletes the metadata from the catalog, but doesn't affect the data files.\n",
        "\n"
      ],
      "metadata": {
        "id": "ZfERuPdkMG2L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Creating catalog tables\n",
        "\n",
        "There are several ways to create catalog tables.\n",
        "\n"
      ],
      "metadata": {
        "id": "rhRODgDjMGz7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Creating a catalog table from a dataframe\n",
        "\n",
        "You can create managed tables by writing a dataframe using the `saveAsTable` operation as shown in the following examples:\n",
        "\n",
        "```python\n",
        "# Save a dataframe as a managed table\n",
        "df.write.format(\"delta\").saveAsTable(\"MyManagedTable\")\n",
        "\n",
        "## specify a path option to save as an external table\n",
        "df.write.format(\"delta\").option(\"path\", \"/mydata\").saveAsTable(\"MyExternalTable\")\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "2FtEURKNMGxv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Creating a catalog table using SQL\n",
        "\n",
        "You can also create a catalog table by using the `CREATE TABLE` SQL statement with the `USING DELTA` clause, and an optional `LOCATION` parameter for external tables. You can run the statement using the SparkSQL API, like the following example:\n",
        "\n",
        "```python\n",
        "spark.sql(\"CREATE TABLE MyExternalTable USING DELTA LOCATION '/mydata'\")\n",
        "```\n",
        "\n",
        "Alternatively you can use the native SQL support in Spark to run the statement:\n",
        "\n",
        "```sql\n",
        "%%sql\n",
        "\n",
        "CREATE TABLE MyExternalTable\n",
        "USING DELTA\n",
        "LOCATION '/mydata'\n",
        "```\n",
        "\n",
        "> **Tip:** The `CREATE TABLE` statement returns an error if a table with the specified name already exists in the catalog. To mitigate this behavior, you can use a `CREATE TABLE IF NOT EXISTS` statement or the `CREATE OR REPLACE TABLE` statement.\n",
        "\n"
      ],
      "metadata": {
        "id": "cB5Mx_BbMGvg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Defining the table schema\n",
        "\n",
        "In all of the examples so far, the table is created without an explicit schema. In the case of tables created by writing a dataframe, the table schema is inherited from the dataframe. When creating an external table, the schema is inherited from any files that are currently stored in the table location. However, when creating a new managed table, or an external table with a currently empty location, you define the table schema by specifying the column names, types, and nullability as part of the CREATE TABLE statement; as shown in the following example:\n",
        "\n",
        "```sql\n",
        "%%sql\n",
        "\n",
        "CREATE TABLE ManagedSalesOrders\n",
        "(\n",
        "    Orderid INT NOT NULL,\n",
        "    OrderDate TIMESTAMP NOT NULL,\n",
        "    CustomerName STRING,\n",
        "    SalesTotal FLOAT NOT NULL\n",
        ")\n",
        "USING DELTA\n",
        "```\n",
        "\n",
        "When using Delta Lake, table schemas are enforced - all inserts and updates must comply with the specified column nullability and data types.\n",
        "\n"
      ],
      "metadata": {
        "id": "2qozG8yzMGtQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Using the DeltaTableBuilder API\n",
        "\n",
        "You can use the DeltaTableBuilder API (part of the Delta Lake API) to create a catalog table, as shown in the following example:\n",
        "\n",
        "```python\n",
        "from delta.tables import *\n",
        "\n",
        "DeltaTable.create(spark) \\\n",
        "  .tableName(\"default.ManagedProducts\") \\\n",
        "  .addColumn(\"Productid\", \"INT\") \\\n",
        "  .addColumn(\"ProductName\", \"STRING\") \\\n",
        "  .addColumn(\"Category\", \"STRING\") \\\n",
        "  .addColumn(\"Price\", \"FLOAT\") \\\n",
        "  .execute()\n",
        "```\n",
        "\n",
        "Similarly to the `CREATE TABLE` SQL statement, the `create` method returns an error if a table with the specified name already exists. You can mitigate this behavior by using the `createIfNotExists` or `createOrReplace` method.\n",
        "\n"
      ],
      "metadata": {
        "id": "hH0mqF551kWu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Using catalog tables\n",
        "\n",
        "You can use catalog tables like tables in any SQL-based relational database, querying and manipulating them by using standard SQL statements. For example, the following code example uses a `SELECT` statement to query the **ManagedSalesOrders** table:\n",
        "\n",
        "```sql\n",
        "%%sql\n",
        "\n",
        "SELECT orderid, salestotal\n",
        "FROM ManagedSalesOrders\n",
        "```\n",
        "\n",
        "> **Tip:** For more information about working with Delta Lake, see [Table batch reads and writes](https://docs.delta.io/latest/delta-batch.html) in the Delta Lake documentation.\n",
        "\n"
      ],
      "metadata": {
        "id": "_szc1AMq1kT1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Next unit: Use Delta Lake with streaming data\n",
        "\n"
      ],
      "metadata": {
        "id": "XvHz1FKU1kRS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unit 5 of 9\n",
        "\n"
      ],
      "metadata": {
        "id": "1ETr_a-D1kO7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Use Delta Lake with streaming data\n",
        "\n",
        "All of the data we've explored up to this point has been static data in files. However, many data analytics scenarios involve *streaming* data that must be processed in near real time. For example, you might need to capture readings emitted by internet-of-things (IoT) devices and store them in a table as they occur.\n",
        "\n"
      ],
      "metadata": {
        "id": "1dOAlnUO1kMW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Spark Structured Streaming\n",
        "\n",
        "A typical stream processing solution involves constantly reading a stream of data from a *source*, optionally processing it to select specific fields, aggregate and group values, or otherwise manipulate the data, and writing the results to a *sink*.\n",
        "\n",
        "Spark includes native support for streaming data through *Spark Structured Streaming*, an API that is based on a boundless dataframe in which streaming data is captured for processing. A Spark Structured Streaming dataframe can read data from many different kinds of streaming source, including network ports, real time message brokering services such as Azure Event Hubs or Kafka, or file system locations.\n",
        "\n",
        "> **Tip:** For more information about Spark Structured Streaming, see [Structured Streaming Programming Guide](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html) in the Spark documentation.\n",
        "\n"
      ],
      "metadata": {
        "id": "0XqRphjp1kJ1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Streaming with Delta Lake tables\n",
        "\n",
        "You can use a Delta Lake table as a source or a sink for Spark Structured Streaming. For example, you could capture a stream of real time data from an IoT device and write the stream directly to a Delta Lake table as a sink - enabling you to query the table to see the latest streamed data. Or, you could read a Delta Table as a streaming source, enabling you to constantly report new data as it is added to the table.\n",
        "\n"
      ],
      "metadata": {
        "id": "jjrKEteJNS5X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Using a Delta Lake table as a streaming source\n",
        "\n",
        "In the following PySpark example, a Delta Lake table is used to store details of Internet sales orders. A stream is created that reads data from the Delta Lake table folder as new data is appended.\n",
        "\n",
        "```python\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "# Load a streaming dataframe from the Delta Table\n",
        "stream_df = spark.readStream.format(\"delta\") \\\n",
        "    .option(\"ignoreChanges\", \"true\") \\\n",
        "    .load(\"/delta/internetorders\")\n",
        "\n",
        "# Now you can process the streaming data in the dataframe\n",
        "# for example, show it:\n",
        "stream_df.writeStream \\\n",
        "    .outputMode(\"append\") \\\n",
        "    .format(\"console\") \\\n",
        "    .start()\n",
        "```\n",
        "\n",
        "> **Note:** When using a Delta Lake table as a streaming source, only *append* operations can be included in the stream. Data modifications will cause an error unless you specify the `ignoreChanges` or `ignoreDeletes` option.\n",
        "\n",
        "After reading the data from the Delta Lake table into a streaming dataframe, you can use the Spark Structured Streaming API to process it. In the example above, the dataframe is simply displayed; but you could use Spark Structured Streaming to aggregate the data over temporal windows (for example to count the number of orders placed every minute) and send the aggregated results to a downstream process for near-real-time visualization.\n",
        "\n"
      ],
      "metadata": {
        "id": "HTqe5B4qNS26"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Using a Delta Lake table as a streaming sink\n",
        "\n",
        "In the following PySpark example, a stream of data is read from JSON files in a folder. The JSON data in each file contains the status for an IoT device in the format `{\"device\":\"Dev1\",\"status\":\"ok\"}` New data is added to the stream whenever a file is added to the folder. The input stream is a boundless dataframe, which is then written in delta format to a folder location for a Delta Lake table.\n",
        "\n",
        "```python\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "# Create a stream that reads JSON data from a folder\n",
        "inputPath = '/streamingdata/'\n",
        "jsonSchema = StructType([\n",
        "    StructField(\"device\", StringType(), False),\n",
        "    StructField(\"status\", StringType(), False)\n",
        "])\n",
        "stream_df = spark.readStream.schema(jsonSchema).option(\"maxFilesPerTrigger\", 1).json(inputPath)\n",
        "\n",
        "# Write the stream to a delta table\n",
        "table_path = '/delta/devicetable'\n",
        "checkpoint_path = '/delta/checkpoint'\n",
        "delta_stream = stream_df.writeStream.format(\"delta\").option(\"checkpointLocation\", checkpoint_path).start(table_path)\n",
        "```\n",
        "\n",
        "> **Note:** The `checkpointLocation` option is used to write a checkpoint file that tracks the state of the stream processing. This file enables you to recover from failure at the point where stream processing left off.\n",
        "\n",
        "After the streaming process has started, you can query the Delta Lake table to which the streaming output is being written to see the latest data. For example, the following code creates a catalog table for the Delta Lake table folder and queries it:\n",
        "\n",
        "```sql\n",
        "%%sql\n",
        "\n",
        "CREATE TABLE DeviceTable\n",
        "USING DELTA\n",
        "LOCATION '/delta/devicetable';\n",
        "\n",
        "SELECT device, status\n",
        "FROM DeviceTable;\n",
        "```\n",
        "\n",
        "To stop the stream of data being written to the Delta Lake table, you can use the `stop` method of the streaming query:\n",
        "\n",
        "```python\n",
        "delta_stream.stop()\n",
        "```\n",
        "\n",
        "> **Tip:** For more information about using Delta Lake tables for streaming data, see [Table streaming reads and writes](https://docs.delta.io/latest/delta-streaming.html) in the Delta Lake documentation.\n",
        "\n"
      ],
      "metadata": {
        "id": "8kN-U-ExNS0c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Next unit: Use Delta Lake in a SQL pool\n",
        "\n"
      ],
      "metadata": {
        "id": "HMIUSz3zNSx6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unit 6 of 9\n",
        "\n"
      ],
      "metadata": {
        "id": "3PUcUMapNSvi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Use Delta Lake in a SQL pool\n",
        "\n",
        "Delta Lake is designed as a transactional, relational storage layer for Apache Spark; including Spark pools in Azure Synapse Analytics. However, Azure Synapse Analytics also includes a serverless SQL pool runtime that enables data analysts and engineers to run SQL queries against data in a data lake or a relational database.\n",
        "\n",
        "> **Note:** You can only *query* data from Delta Lake tables in a serverless SQL pool; you can't *update*, *insert*, or *delete* data.\n",
        "\n"
      ],
      "metadata": {
        "id": "3_khGSfeNSrb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Querying delta formatted files with OPENROWSET\n",
        "\n",
        "The serverless SQL pool in Azure Synapse Analytics includes support for reading delta format files; enabling you to use the SQL pool to query Delta Lake tables. This approach can be useful in scenarios where you want to use Spark and Delta tables to process large quantities of data, but use the SQL pool to run queries for reporting and analysis of the processed data.\n",
        "\n",
        "In the following example, a SQL `SELECT` query reads delta format data using the `OPENROWSET` function.\n",
        "\n",
        "```sql\n",
        "SELECT *\n",
        "FROM\n",
        "    OPENROWSET(\n",
        "        BULK 'https://mystore.dfs.core.windows.net/files/delta/mytable/',\n",
        "        FORMAT = 'DELTA'\n",
        "    ) AS deltadata\n",
        "```\n",
        "\n",
        "You could run this query in a serverless SQL pool to retrieve the latest data from the Delta Lake table stored in the specified file location.\n",
        "\n",
        "You could also create a database and add a data source that encapsulates the location of your Delta Lake data files, as shown in this example:\n",
        "\n",
        "```sql\n",
        "CREATE DATABASE MyDB\n",
        "      COLLATE Latin1_General_100_BIN2_UTF8;\n",
        "GO;\n",
        "\n",
        "USE MyDB;\n",
        "GO\n",
        "\n",
        "CREATE EXTERNAL DATA SOURCE DeltaLakeStore\n",
        "WITH\n",
        "(\n",
        "    LOCATION = 'https://mystore.dfs.core.windows.net/files/delta/'\n",
        ");\n",
        "GO\n",
        "\n",
        "SELECT TOP 10 *\n",
        "FROM OPENROWSET(\n",
        "        BULK 'mytable',\n",
        "        DATA_SOURCE = 'DeltaLakeStore',\n",
        "        FORMAT = 'DELTA'\n",
        "    ) as deltadata;\n",
        "```\n",
        "\n",
        "> **Note:** When working with Delta Lake data, which is stored in Parquet format, it's generally best to create a database with a UTF-8 based collation in order to ensure string compatibility.\n",
        "\n"
      ],
      "metadata": {
        "id": "LEKxb7LINSpW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Querying catalog tables\n",
        "\n",
        "The serverless SQL pool in Azure Synapse Analytics has shared access to databases in the Spark metastore, so you can query catalog tables that were created using Spark SQL. In the following example, a SQL query in a serverless SQL pool queries a catalog table that contains Delta Lake data:\n",
        "\n",
        "```sql\n",
        "-- By default, Spark catalog tables are created in a database named \"default\"\n",
        "-- If you created another database using Spark SQL, you can use it here\n",
        "USE default;\n",
        "\n",
        "SELECT * FROM MyDeltaTable;\n",
        "```\n",
        "\n",
        "> **Tip:** For more information about using Delta Tables from a serverless SQL pool, see [Query Delta Lake files using serverless SQL pool in Azure Synapse Analytics](https://learn.microsoft.com/en-us/azure/synapse-analytics/sql/query-delta-lake-format) in the Azure Synapse Analytics documentation.\n",
        "\n"
      ],
      "metadata": {
        "id": "EFcIG9BdNkcX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unit 7 of 9\n",
        "\n"
      ],
      "metadata": {
        "id": "U49nMO6DNkZ7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise - Use Delta Lake in Azure Synapse Analytics\n",
        "\n",
        "Now it's your chance to explore Delta Lake for yourself. In this exercise, you'll use a Spark pool in Azure Synapse Analytics to create and query Delta Lake tables, and query Delta Lake data from a serverless SQL pool.\n",
        "\n",
        "> **Note:** To complete this lab, you will need an [Azure subscription](https://azure.microsoft.com/free) in which you have administrative access.\n",
        "\n",
        "Launch the exercise and follow the instructions.\n",
        "\n",
        "[Launch Exercise](https://aka.ms/mslearn-delta-lake)\n",
        "\n"
      ],
      "metadata": {
        "id": "oQEssg0SNkXh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Next unit: Knowledge check\n",
        "\n"
      ],
      "metadata": {
        "id": "bSFAQtg4NkVI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unit 8 of 9\n",
        "\n"
      ],
      "metadata": {
        "id": "saYqwhcvNkS4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Knowledge check\n",
        "\n",
        "1. Which of the following descriptions best fits Delta Lake?\n",
        "\n",
        "    - ☐ A Spark API for exporting data from a relational database into CSV files.\n",
        "    - ☑ A relational storage layer for Spark that supports tables based on Parquet files.\n",
        "    > Correct. Delta Lake provides a relational storage layer in which you can create tables based on Parquet files in a data lake.\n",
        "    - ☐ A synchronization solution that replicates data between SQL pools and Spark pools.\n",
        "\n",
        "2. You've loaded a Spark dataframe with data, that you now want to use in a Delta Lake table. What format should you use to write the dataframe to storage?\n",
        "\n",
        "    - ☐ CSV\n",
        "    - ☐ PARQUET\n",
        "    - ☑ DELTA\n",
        "    > Correct. Storing a dataframe in DELTA format creates parquet files for the data and the transaction log metadata necessary for Delta Lake tables.\n",
        "\n",
        "3. What feature of Delta Lake enables you to retrieve data from previous versions of a table?\n",
        "\n",
        "    - ☐ Spark Structured Streaming\n",
        "    - ☑ Time Travel\n",
        "    > Correct. The Time Travel feature is based on the transaction log, which enables you to specify a version number or timestamp for the data you want to retrieve.\n",
        "    - ☐ Catalog Tables\n",
        "\n",
        "4. You have a managed catalog table that contains Delta Lake data. If you drop the table, what will happen?\n",
        "\n",
        "    - ☑ The table metadata and data files will be deleted.\n",
        "    > Correct. The life-cycle of the metadata and data for a managed table are the same.\n",
        "    - ☐ The table metadata will be removed from the catalog, but the data files will remain intact.\n",
        "    - ☐ The table metadata will remain in the catalog, but the data files will be deleted.\n",
        "\n",
        "5. When using Spark Structured Streaming, a Delta Lake table can be which of the following?\n",
        "\n",
        "    - ☐ Only a source\n",
        "    - ☐ Only a sink\n",
        "    - ☑ Either a source or a sink\n",
        "    > Correct. A Delta Lake table can be a source or a sink.\n",
        "\n"
      ],
      "metadata": {
        "id": "zqc-BRnoNkQc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Next unit: Summary\n",
        "\n"
      ],
      "metadata": {
        "id": "4MJhPAbcNxwS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unit 9 of 9\n",
        "\n"
      ],
      "metadata": {
        "id": "2MREEb2VNxsK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Summary\n",
        "\n",
        "Delta Lake is an increasingly used technology for large-scale data analytics where you need to combine the flexibility and scalability of a data lake with the transactional consistency and structure of a relational database.\n",
        "\n",
        "In this module, you learned how to:\n",
        "\n",
        "- Describe core features and capabilities of Delta Lake.\n",
        "- Create and use Delta Lake tables in a Synapse Analytics Spark pool.\n",
        "- Create Spark catalog tables for Delta Lake data.\n",
        "- Use Delta Lake tables for streaming data.\n",
        "- Query Delta Lake tables from a Synapse Analytics SQL pool.\n",
        "\n",
        "To learn more about using Delta Lake in Azure Synapse Analytics, see [Linux Foundation Delta Lake overview](https://learn.microsoft.com/en-us/azure/synapse-analytics/spark/apache-spark-delta-lake-overview) in the Azure Synapse Analytics documentation.\n",
        "\n"
      ],
      "metadata": {
        "id": "tya7m7PINxpz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### All units complete\n",
        "\n"
      ],
      "metadata": {
        "id": "fjgaGmzpNxne"
      }
    }
  ]
}