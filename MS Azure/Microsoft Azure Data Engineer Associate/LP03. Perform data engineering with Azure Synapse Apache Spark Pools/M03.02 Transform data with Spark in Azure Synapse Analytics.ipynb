{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# M03.02 Transform data with Spark in Azure Synapse Analytics\n",
        "\n"
      ],
      "metadata": {
        "id": "WPAFq47kRiNe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unit 1 of 7\n",
        "\n"
      ],
      "metadata": {
        "id": "cChZ81z5RnJy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Introduction\n",
        "\n",
        "Apache Spark provides a powerful platform for performing data cleansing and transformation tasks on large volumes of data. By using the Spark *dataframe* object, you can easily load data from files in a data lake and perform complex modifications. You can then save the transformed data back to the data lake for downstream processing or ingestion into a data warehouse.\n",
        "\n",
        "Azure Synapse Analytics provides Apache Spark pools that you can use to run Spark workloads to transform data as part of a data ingestion and preparation workload. You can use natively supported notebooks to write and run code on a Spark pool to prepare data for analysis. You can then use other Azure Synapse Analytics capabilities such as SQL pools to work with the transformed data.\n",
        "\n"
      ],
      "metadata": {
        "id": "8dYmH0i5RnHG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Next unit: Modify and save dataframes\n",
        "\n"
      ],
      "metadata": {
        "id": "vzOqzAIMRnEO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unit 2 of 7\n",
        "\n"
      ],
      "metadata": {
        "id": "jpBbnxVkRnCG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Modify and save dataframes\n",
        "\n",
        "Apache Spark provides the *dataframe* object as the primary structure for working with data. You can use dataframes to query and transform data, and persist the results in a data lake. To load data into a dataframe, you use the **spark.read** function, specifying the file format, path, and optionally the schema of the data to be read. For example, the following code loads data from all .csv files in the **orders** folder into a dataframe named **order_details** and then displays the first five records.\n",
        "\n",
        "```python\n",
        "order_details = spark.read.csv('/orders/*.csv', header=True, inferSchema=True)\n",
        "display(order_details.limit(5))\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "R5Q7xlmoRm-6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Transform the data structure\n",
        "\n",
        "After loading the source data into a dataframe, you can use the dataframe object's methods and Spark functions to transform it. Typical operations on a dataframe include:\n",
        "\n",
        "- Filtering rows and columns\n",
        "- Renaming columns\n",
        "- Creating new columns, often derived from existing ones\n",
        "- Replacing null or other values\n",
        "\n",
        "In the following example, the code uses the `split` function to separate the values in the **CustomerName** column into two new columns named **FirstName** and **LastName**. Then it uses the `drop` method to delete the original **CustomerName** column.\n",
        "\n",
        "```python\n",
        "from pyspark.sql.functions import split, col\n",
        "\n",
        "# Create the new FirstName and LastName fields\n",
        "transformed_df = order_details.withColumn(\"FirstName\", split(col(\"CustomerName\"), \" \").getItem(0)).withColumn(\"LastName\", split(col(\"CustomerName\"), \" \").getItem(1))\n",
        "\n",
        "# Remove the CustomerName field\n",
        "transformed_df = transformed_df.drop(\"CustomerName\")\n",
        "\n",
        "display(transformed_df.limit(5))\n",
        "```\n",
        "\n",
        "You can use the full power of the Spark SQL library to transform the data by filtering rows, deriving, removing, renaming columns, and any applying other required data modifications.\n",
        "\n"
      ],
      "metadata": {
        "id": "hOO1p2h-Rm8e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Save the transformed data\n",
        "\n",
        "After your dataFrame is in the required structure, you can save the results to a supported format in your data lake.\n",
        "\n",
        "The following code example saves the dataFrame into a *parquet* file in the data lake, replacing any existing file of the same name.\n",
        "\n",
        "```python\n",
        "transformed_df.write.mode(\"overwrite\").parquet('/transformed_data/orders.parquet')\n",
        "print (\"Transformed data saved!\")\n",
        "```\n",
        "\n",
        "> **Note:** The Parquet format is typically preferred for data files that you will use for further analysis or ingestion into an analytical store. Parquet is a very efficient format that is supported by most large scale data analytics systems. In fact, sometimes your data transformation requirement may simply be to convert data from another format (such as CSV) to Parquet!\n",
        "\n"
      ],
      "metadata": {
        "id": "R8DPPRyCRm51"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Next unit: Partition data files\n",
        "\n"
      ],
      "metadata": {
        "id": "nVqEMjq6Rm3W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unit 3 of 7\n",
        "\n"
      ],
      "metadata": {
        "id": "WSym23gpRm05"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Partition data files\n",
        "\n",
        "Partitioning is an optimization technique that enables spark to maximize performance across the worker nodes. More performance gains can be achieved when filtering data in queries by eliminating unnecessary disk IO.\n",
        "\n"
      ],
      "metadata": {
        "id": "CaQ_3Vv5RmyW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Partition the output file\n",
        "\n",
        "To save a dataframe as a partitioned set of files, use the **partitionBy** method when writing the data.\n",
        "\n",
        "The following example creates a derived **Year** field. Then uses it to partition the data.\n",
        "\n",
        "```python\n",
        "from pyspark.sql.functions import year, col\n",
        "\n",
        "# Load source data\n",
        "df = spark.read.csv('/orders/*.csv', header=True, inferSchema=True)\n",
        "\n",
        "# Add Year column\n",
        "dated_df = df.withColumn(\"Year\", year(col(\"OrderDate\")))\n",
        "\n",
        "# Partition by year\n",
        "dated_df.write.partitionBy(\"Year\").mode(\"overwrite\").parquet(\"/data\")\n",
        "```\n",
        "\n",
        "The folder names generated when partitioning a dataframe include the partitioning column name and value in a **column=value** format, as shown here:\n",
        "\n",
        "![Diagram representing a partitioned file folder structure.](https://learn.microsoft.com/en-us/training/wwl-data-ai/transform-data-spark-azure-synapse-analytics/media/3-partition-data-files.png)\n",
        "\n",
        "> **Note:** You can partition the data by multiple columns, which results in a hierarchy of folders for each partitioning key. For example, you could partition the order in the example by year and month, so that the folder hierarchy includes a folder for each year value, which in turn contains a subfolder for each month value.\n",
        "\n"
      ],
      "metadata": {
        "id": "NZUvIhJXRmwC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Filter parquet files in a query\n",
        "\n",
        "When reading data from parquet files into a dataframe, you have the ability to pull data from any folder within the hierarchical folders. This filtering process is done with the use of explicit values and wildcards against the partitioned fields.\n",
        "\n",
        "In the following example, the following code will pull the sales orders, which were placed in 2020.\n",
        "\n",
        "```python\n",
        "orders_2020 = spark.read.parquet('/partitioned_data/Year=2020')\n",
        "display(orders_2020.limit(5))\n",
        "```\n",
        "\n",
        "> **Note:** The partitioning columns specified in the file path are omitted in the resulting dataframe. The results produced by the example query would not include a **Year** column - all rows would be from 2020.\n",
        "\n"
      ],
      "metadata": {
        "id": "sFXi5QNFRmtq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Next unit: Transform data with SQL\n",
        "\n"
      ],
      "metadata": {
        "id": "TlGq2OYaRmrG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unit 4 of 7\n",
        "\n"
      ],
      "metadata": {
        "id": "g2cl02goRmo2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transform data with SQL\n",
        "\n",
        "The SparkSQL library, which provides the dataframe structure also enables you to use SQL as a way of working with data. With this approach, You can query and transform data in dataframes by using SQL queries, and persist the results as tables.\n",
        "\n",
        "> **Note:** Tables are metadata abstractions over files. The data is not stored in a relational table, but the table provides a relational layer over files in the data lake.\n",
        "\n"
      ],
      "metadata": {
        "id": "-IhK1t-_Rmml"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Define tables and views\n",
        "\n",
        "Table definitions in Spark are stored in the *metastore*, a metadata layer that encapsulates relational abstractions over files. *External* tables are relational tables in the metastore that reference files in a data lake location that you specify. You can access this data by querying the table or by reading the files directly from the data lake.\n",
        "\n",
        "> **Note:** External tables are \"loosely bound\" to the underlying files and deleting the table *does not* delete the files. This allows you to use Spark to do the heavy lifting of transformation then persist the data in the lake. After this is done you can drop the table and downstream processes can access these optimized structures. You can also define *managed* tables, for which the underlying data files are stored in an internally managed storage location associated with the metastore. Managed tables are \"tightly-bound\" to the files, and dropping a managed table deletes the associated files.\n",
        "\n",
        "The following code example saves a dataframe (loaded from CSV files) as an external table name **sales_orders**. The files are stored in the **/sales_orders_table** folder in the data lake.\n",
        "\n",
        "```python\n",
        "order_details.write.saveAsTable('sales_orders', format='parquet', mode='overwrite', path='/sales_orders_table')\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "2VdV2sHHRmkK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Use SQL to query and transform the data\n",
        "\n",
        "After defining a table, you can use of SQL to query and transform its data. The following code creates two new derived columns named **Year** and **Month** and then creates a new table *transformed_orders* with the new derived columns added.\n",
        "\n",
        "```python\n",
        "# Create derived columns\n",
        "sql_transform = spark.sql(\"SELECT *, YEAR(OrderDate) AS Year, MONTH(OrderDate) AS Month FROM sales_orders\")\n",
        "\n",
        "# Save the results\n",
        "sql_transform.write.partitionBy(\"Year\",\"Month\").saveAsTable('transformed_orders', format='parquet', mode='overwrite', path='/transformed_orders_table')\n",
        "```\n",
        "\n",
        "The data files for the new table are stored in a hierarchy of folders with the format of **Year=\\*NNNN\\* / Month=\\*N\\***, with each folder containing a parquet file for the corresponding orders by year and month.\n",
        "\n"
      ],
      "metadata": {
        "id": "STD-_pQFRmhx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Query the metastore\n",
        "\n",
        "Because this new table was created in the metastore, you can use SQL to query it directly with the `%%sql` magic key in the first line to indicate that the SQL syntax will be used as shown in the following script:\n",
        "\n",
        "```sql\n",
        "%%sql\n",
        "\n",
        "SELECT * FROM transformed_orders\n",
        "WHERE Year = 2021\n",
        "    AND Month = 1\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "jVicZkxnRmfZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Drop tables\n",
        "\n",
        "When working with external tables, you can use the `DROP` command to delete the table definitions from the metastore without affecting the files in the data lake. This approach enables you to clean up the metastore after using SQL to transform the data, while making the transformed data files available to downstream data analysis and ingestion processes.\n",
        "\n",
        "```sql\n",
        "%%sql\n",
        "\n",
        "DROP TABLE transformed_orders;\n",
        "DROP TABLE sales_orders;\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "mQG6HOkjRmdG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Next unit: Exercise: Transform data with Spark in Azure Synapse Analytics\n",
        "\n"
      ],
      "metadata": {
        "id": "eOt1-XR_RmbJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unit 5 of 7\n",
        "\n"
      ],
      "metadata": {
        "id": "QhV2Tej0RmYq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise: Transform data with Spark in Azure Synapse Analytics\n",
        "\n",
        "Now it's your chance to use Spark to transform data for yourself. In this exercise, you’ll use a Spark notebook in Azure Synapse Analytics to transform data in files.\n",
        "\n",
        "> **Note:** To complete this lab, you will need an [Azure subscription](https://azure.microsoft.com/free) in which you have administrative access.\n",
        "\n",
        "Launch the exercise and follow the instructions.\n",
        "\n",
        "[Launch Exercise](https://aka.ms/mslearn-transform-data-with-spark)\n",
        "\n"
      ],
      "metadata": {
        "id": "2Pt9VVkdRmWN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Next unit: Knowledge check\n",
        "\n"
      ],
      "metadata": {
        "id": "ggwQ01VwRmT6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unit 6 of 7\n",
        "\n"
      ],
      "metadata": {
        "id": "O8ecg6c1RmRl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Knowledge check\n",
        "\n",
        "1. Which method of the Dataframe object is used to save a dataframe as a file?\n",
        "\n",
        "    - ☐ toFile()\n",
        "    - ☑ write()\n",
        "    > That's correct. The write() method to save a DataFrame as a file.\n",
        "    - ☐ save()\n",
        "\n",
        "2. Which method is used to split the data across folders when saving a dataframe?\n",
        "\n",
        "    - ☐ splitBy()\n",
        "    - ☐ distributeBy()\n",
        "    - ☑ partitionBy()\n",
        "    > That's correct. This method is an expressive and convenient operation in PySpark, which partitions the DataFrame based on given columns and distributes the partitions across the nodes in the cluster.\n",
        "\n",
        "3. What happens if you drop an external table that is based on existing files?\n",
        "\n",
        "    - ☐ An error – you must delete the files first\n",
        "    - ☑ The table is dropped from the metastore but the files remain unaffected\n",
        "    > That's correct. The tables are loosely coupled from the files allowing the table to be dropped while the files remain.\n",
        "    - ☐ The table is dropped from the metastore and the files are deleted\n",
        "\n"
      ],
      "metadata": {
        "id": "v0lWSpidRmPG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Next unit: Summary\n",
        "\n"
      ],
      "metadata": {
        "id": "1n2gg1bIRiLK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unit 7 of 7\n",
        "\n"
      ],
      "metadata": {
        "id": "inf6UTn0RiI6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Summary\n",
        "\n",
        "In this module, you've learned how to use Apache Spark to transform data in Azure Synapse Analytics. Using Spark for complex data transformations is a common technique because of the inherent scalability of the Spark platform. You can use code in notebooks to experiment with data transformations, and then include those notebooks in automated pipelines as part of a data integration solution.\n",
        "\n",
        "> **Tip:** Learn more about using the [Spark SQL and DataFrames Guide](https://spark.apache.org/docs/latest/sql-programming-guide.html).\n",
        "\n"
      ],
      "metadata": {
        "id": "iDrdWR4jRiGm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### All units complete\n",
        "\n"
      ],
      "metadata": {
        "id": "RGvmNJpTRiEG"
      }
    }
  ]
}