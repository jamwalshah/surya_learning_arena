{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# M02.02 Use Azure Synapse serverless SQL pools to transform data in a data lake\n",
        "\n"
      ],
      "metadata": {
        "id": "oi-56UiSUY6l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unit 1 of 7\n",
        "\n"
      ],
      "metadata": {
        "id": "8H0P4pHZUY4d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Introduction\n",
        "\n",
        "While SQL is commonly used by data analysts to query data and support analytical and reporting workloads, data engineers often need to use SQL to *transform* data; often as part of a data ingestion pipeline or extract, transform, and load (ETL) process.\n",
        "\n",
        "In this module, you'll learn how to use `CREATE EXTERNAL TABLE AS SELECT` (CETAS) statements to transform data, and store the results in files in a data lake that can be queried through a relational table in a serverless SQL database or processed directly from the file system.\n",
        "\n",
        "After completing this module, you'll be able to:\n",
        "\n",
        "- Use a CREATE EXTERNAL TABLE AS SELECT (CETAS) statement to transform data.\n",
        "- Encapsulate a CETAS statement in a stored procedure.\n",
        "- Include a data transformation stored procedure in a pipeline.\n",
        "\n"
      ],
      "metadata": {
        "id": "wL14TafCUY2Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Prerequisites\n",
        "\n",
        "Before starting this module, you should have the following prerequisite skills and knowledge:\n",
        "\n",
        "- Familiarity with Azure Synapse Analytics.\n",
        "- Experience using Transact-SQL to query and manipulate data.\n",
        "\n"
      ],
      "metadata": {
        "id": "RHxk0OGvUYz7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Next unit: Transform data files with the CREATE EXTERNAL TABLE AS SELECT statement\n",
        "\n"
      ],
      "metadata": {
        "id": "wZzosQE8UYvj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unit 2 of 7\n",
        "\n"
      ],
      "metadata": {
        "id": "XtMVY4mzUYtP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transform data files with the CREATE EXTERNAL TABLE AS SELECT statement\n",
        "\n",
        "The SQL language includes many features and functions that enable you to manipulate data. For example, you can use SQL to:\n",
        "\n",
        "- Filter rows and columns in a dataset.\n",
        "- Rename data fields and convert between data types.\n",
        "- Calculate derived data fields.\n",
        "- Manipulate string values.\n",
        "- Group and aggregate data\n",
        "\n",
        "Azure Synapse serverless SQL pools can be used to run SQL statements that transform data and persist the results as a file in a data lake for further processing or querying. If you're familiar with Transact-SQL syntax, you can craft a SELECT statement that applies the specific transformation you're interested in, and store the results of the SELECT statement in a selected file format with a metadata table schema that can be queried using SQL.\n",
        "\n",
        "You can use a CREATE EXTERNAL TABLE AS SELECT (CETAS) statement in a dedicated SQL pool or serverless SQL pool to persist the results of a query in an external table, which stores its data in a file in the data lake.\n",
        "\n",
        "The CETAS statement includes a SELECT statement that queries and manipulates data from any valid data source (which could be an existing table or view in a database, or an OPENROWSET function that reads file-based data from the data lake). The results of the SELECT statement are then persisted in an external table, which is a metadata object in a database that provides a relational abstraction over data stored in files. The following diagram illustrates the concept visually:\n",
        "\n",
        "![A diagram showing a CREATE EXTERNAL TABLE AS SELECT statement saving query results as a file.](https://learn.microsoft.com/en-us/training/wwl-data-ai/use-azure-synapse-serverless-sql-pools-for-transforming-data-lake/media/create-external-table-as-select.png)\n",
        "\n",
        "By applying this technique, you can use SQL to extract and transform data from files or tables, and store the transformed results for downstream processing or analysis. Subsequent operations on the transformed data can be performed against the relational table in the SQL pool database or directly against the underlying data files.\n",
        "\n"
      ],
      "metadata": {
        "id": "hl1_e3LwUYrR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Creating external database objects to support CETAS\n",
        "\n",
        "To use CETAS expressions, you must create the following types of object in a database for either a serverless or dedicated SQL pool. When using a serverless SQL pool, create these objects in a custom database (created using the `CREATE DATABASE` statement), not the built-in database.\n",
        "\n"
      ],
      "metadata": {
        "id": "Zj7pu58KUYo3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### External data source\n",
        "\n",
        "An external data source encapsulates a connection to a file system location in a data lake. You can then use this connection to specify a relative path in which the data files for the external table created by the CETAS statement are saved.\n",
        "\n",
        "If the source data for the CETAS statement is in files in the same data lake path, you can use the same external data source in the OPENROWSET function used to query it. Alternatively, you can create a separate external data source for the source files or use a fully qualified file path in the OPENROWSET function.\n",
        "\n",
        "To create an external data source, use the `CREATE EXTERNAL DATA SOURCE` statement, as shown in this example:\n",
        "\n",
        "```sql\n",
        "-- Create an external data source for the Azure storage account\n",
        "CREATE EXTERNAL DATA SOURCE files\n",
        "WITH (\n",
        "    LOCATION = 'https://mydatalake.blob.core.windows.net/data/files/',\n",
        "    TYPE = HADOOP, -- For dedicated SQL pool\n",
        "    -- TYPE = BLOB_STORAGE, -- For serverless SQL pool\n",
        "    CREDENTIAL = storageCred\n",
        ");\n",
        "```\n",
        "\n",
        "The previous example assumes that users running queries that use the external data source will have sufficient permissions to access the files. An alternative approach is to encapsulate a credential in the external data source so that it can be used to access file data without granting all users permissions to read it directly:\n",
        "\n",
        "```sql\n",
        "CREATE DATABASE SCOPED CREDENTIAL storagekeycred\n",
        "WITH\n",
        "    IDENTITY='SHARED ACCESS SIGNATURE',  \n",
        "    SECRET = 'sv=xxx...';\n",
        "\n",
        "CREATE EXTERNAL DATA SOURCE secureFiles\n",
        "WITH (\n",
        "    LOCATION = 'https://mydatalake.blob.core.windows.net/data/secureFiles/'\n",
        "    CREDENTIAL = storagekeycred\n",
        ");\n",
        "```\n",
        "\n",
        "> **Tip:** In addition to SAS authentication, you can define credentials that use *managed identity* (the Microsoft Entra identity used by your Azure Synapse workspace), a specific Microsoft Entra principal, or passthrough authentication based on the identity of the user running the query (which is the default type of authentication). To learn more about using credentials in a serverless SQL pool, see the [Control storage account access for serverless SQL pool in Azure Synapse Analytics](https://learn.microsoft.com/en-us/azure/synapse-analytics/sql/develop-storage-files-storage-access-control) article in Azure Synapse Analytics documentation.\n",
        "\n"
      ],
      "metadata": {
        "id": "PvdZ84ZsUYmj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### External file format\n",
        "\n",
        "The CETAS statement creates a table with its data stored in files. You must specify the format of the files you want to create as an external file format.\n",
        "\n",
        "To create an external file format, use the `CREATE EXTERNAL FILE FORMAT` statement, as shown in this example:\n",
        "\n",
        "```sql\n",
        "CREATE EXTERNAL FILE FORMAT ParquetFormat\n",
        "WITH (\n",
        "        FORMAT_TYPE = PARQUET,\n",
        "        DATA_COMPRESSION = 'org.apache.hadoop.io.compress.SnappyCodec'\n",
        "    );\n",
        "```\n",
        "\n",
        "> **Tip:** In this example, the files will be saved in Parquet format. You can also create external file formats for other types of file. See [CREATE EXTERNAL FILE FORMAT (Transact-SQL)](https://learn.microsoft.com/en-us/sql/t-sql/statements/create-external-file-format-transact-sql) for details.\n",
        "\n"
      ],
      "metadata": {
        "id": "_p4DgJetUYkT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Using the CETAS statement\n",
        "\n",
        "After creating an external data source and external file format, you can use the CETAS statement to transform data and stored the results in an external table.\n",
        "\n",
        "For example, suppose the source data you want to transform consists of sales orders in comma-delimited text files that are stored in a folder in a data lake. You want to filter the data to include only orders that are marked as \"special order\", and save the transformed data as Parquet files in a different folder in the same data lake. You could use the same external data source for both the source and destination folders as shown in this example:\n",
        "\n",
        "```sql\n",
        "CREATE EXTERNAL TABLE SpecialOrders\n",
        "    WITH (\n",
        "        -- details for storing results\n",
        "        LOCATION = 'special_orders/',\n",
        "        DATA_SOURCE = files,\n",
        "        FILE_FORMAT = ParquetFormat\n",
        "    )\n",
        "AS\n",
        "SELECT OrderID, CustomerName, OrderTotal\n",
        "FROM\n",
        "    OPENROWSET(\n",
        "        -- details for reading source files\n",
        "        BULK 'sales_orders/*.csv',\n",
        "        DATA_SOURCE = 'files',\n",
        "        FORMAT = 'CSV',\n",
        "        PARSER_VERSION = '2.0',\n",
        "        HEADER_ROW = TRUE\n",
        "    ) AS source_data\n",
        "WHERE OrderType = 'Special Order';\n",
        "```\n",
        "\n",
        "The `LOCATION` and `BULK` parameters in the previous example are relative paths for the results and source files respectively. The paths are relative to the file system location referenced by the **files** external data source.\n",
        "\n",
        "An important point to understand is that you **must** use an external data source to specify the location where the transformed data for the external table is to be saved. When file-based source data is stored in the same folder hierarchy, you can use the same external data source. Otherwise, you can use a second data source to define a connection to the source data or use the fully qualified path, as shown in this example:\n",
        "\n",
        "```sql\n",
        "CREATE EXTERNAL TABLE SpecialOrders\n",
        "    WITH (\n",
        "        -- details for storing results\n",
        "        LOCATION = 'special_orders/',\n",
        "        DATA_SOURCE = files,\n",
        "        FILE_FORMAT = ParquetFormat\n",
        "    )\n",
        "AS\n",
        "SELECT OrderID, CustomerName, OrderTotal\n",
        "FROM\n",
        "    OPENROWSET(\n",
        "        -- details for reading source files\n",
        "        BULK 'https://mystorage.blob.core.windows.net/data/sales_orders/*.csv',\n",
        "        FORMAT = 'CSV',\n",
        "        PARSER_VERSION = '2.0',\n",
        "        HEADER_ROW = TRUE\n",
        "    ) AS source_data\n",
        "WHERE OrderType = 'Special Order';\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "AYcw34xoUYh7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Dropping external tables\n",
        "\n",
        "If you no longer need the external table containing the transformed data, you can drop it from the database by using the `DROP EXTERNAL TABLE` statement, as shown here:\n",
        "\n",
        "```sql\n",
        "DROP EXTERNAL TABLE SpecialOrders;\n",
        "```\n",
        "\n",
        "However, it's important to understand that external tables are a metadata abstraction over the files that contain the actual data. Dropping an external table does ***not*** delete the underlying files.\n",
        "\n"
      ],
      "metadata": {
        "id": "bgJ7AP-QUYfj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Next unit: Encapsulate data transformations in a stored procedure\n",
        "\n"
      ],
      "metadata": {
        "id": "3-RX_zQ_UYdT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unit 3 of 7\n",
        "\n"
      ],
      "metadata": {
        "id": "uipBkgAeUYbE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encapsulate data transformations in a stored procedure\n",
        "\n",
        "While you can run a `CREATE EXTERNAL TABLE AS SELECT` (CETAS) statement in a script whenever you need to transform data, it's good practice to encapsulate the transformation operation in stored procedure. This approach can make it easier to operationalize data transformations by enabling you to supply parameters, retrieve outputs, and include additional logic in a single procedure call.\n",
        "\n",
        "For example, the following code creates a stored procedure that drops the external table if it already exists before recreating it with order data for the specified year:\n",
        "\n",
        "```sql\n",
        "CREATE PROCEDURE usp_special_orders_by_year @order_year INT\n",
        "AS\n",
        "BEGIN\n",
        "\n",
        "    -- Drop the table if it already exists\n",
        "    IF EXISTS (\n",
        "                SELECT * FROM sys.external_tables\n",
        "                WHERE name = 'SpecialOrders'\n",
        "            )\n",
        "        DROP EXTERNAL TABLE SpecialOrders\n",
        "\n",
        "    -- Create external table with special orders\n",
        "    -- from the specified year\n",
        "    CREATE EXTERNAL TABLE SpecialOrders\n",
        "        WITH (\n",
        "            LOCATION = 'special_orders/',\n",
        "            DATA_SOURCE = files,\n",
        "            FILE_FORMAT = ParquetFormat\n",
        "        )\n",
        "    AS\n",
        "    SELECT OrderID, CustomerName, OrderTotal\n",
        "    FROM\n",
        "        OPENROWSET(\n",
        "            BULK 'sales_orders/*.csv',\n",
        "            DATA_SOURCE = 'files',\n",
        "            FORMAT = 'CSV',\n",
        "            PARSER_VERSION = '2.0',\n",
        "            HEADER_ROW = TRUE\n",
        "        ) AS source_data\n",
        "    WHERE OrderType = 'Special Order'\n",
        "    AND YEAR(OrderDate) = @order_year\n",
        "END\n",
        "```\n",
        "\n",
        "> Note: As discussed previously, dropping an existing external table does not delete the folder containing its data files. You must explicitly delete the target folder if it exists before running the stored procedure, or an error will occur.\n",
        "\n",
        "In addition to encapsulating Transact-SQL logic, stored procedures also provide the following benefits:\n",
        "\n"
      ],
      "metadata": {
        "id": "SYpCf0WxUYYv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Reduces client to server network traffic\n",
        "\n",
        "The commands in a procedure are executed as a single batch of code; which can significantly reduce network traffic between the server and client because only the call to execute the procedure is sent across the network.\n",
        "\n"
      ],
      "metadata": {
        "id": "1kvHlDVoUYWp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Provides a security boundary\n",
        "\n",
        "Multiple users and client programs can perform operations on underlying database objects through a procedure, even if the users and programs don't have direct permissions on those underlying objects. The procedure controls what processes and activities are performed and protects the underlying database objects; eliminating the requirement to grant permissions at the individual object level and simplifies the security layers.\n",
        "\n"
      ],
      "metadata": {
        "id": "vApUfUmhUYUf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Eases maintenance\n",
        "\n",
        "Any changes in the logic or file system locations involved in the data transformation can be applied only to the stored procedure; without requiring updates to client applications or other calling functions.\n",
        "\n"
      ],
      "metadata": {
        "id": "y6JGXBkIUYSM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Improved performance\n",
        "\n",
        "Stored procedures are compiled the first time they're executed, and the resulting execution plan is held in the cache and reused on subsequent runs of the same stored procedure. As a result, it takes less time to process the procedure.\n",
        "\n"
      ],
      "metadata": {
        "id": "PDMx4EFejE0A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Next unit: Include a data transformation stored procedure in a pipeline\n",
        "\n"
      ],
      "metadata": {
        "id": "R1yofqK9jEws"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unit 4 of 7\n",
        "\n"
      ],
      "metadata": {
        "id": "MMcfSMgkjEqS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Include a data transformation stored procedure in a pipeline\n",
        "\n",
        "Encapsulating a `CREATE EXTERNAL TABLE AS SELECT` (CETAS) statement in a stored procedure makes it easier for you to operationalize data transformations that you may need to perform repeatedly. In Azure Synapse Analytics and Azure Data Factory, you can create pipelines that connect to *linked services*, including Azure Data Lake Store Gen2 storage accounts that host data lake files, and serverless SQL pools; enabling you to call your stored procedures as part of an overall data extract, transform, and load (ETL) pipeline.\n",
        "\n",
        "For example, you can create a pipeline that includes the following activities:\n",
        "\n",
        "- A **Delete** activity that deletes the target folder for the transformed data in the data lake if it already exists.\n",
        "- A **Stored procedure** activity that connects to your serverless SQL pool and runs the stored procedure that encapsulates your CETAS operation.\n",
        "\n",
        "![A screenshot of a pipeline containing a Delete activity and a Stored procedure activity.](https://learn.microsoft.com/en-us/training/wwl-data-ai/use-azure-synapse-serverless-sql-pools-for-transforming-data-lake/media/synapse-pipeline.png)\n",
        "\n",
        "Creating a pipeline for the data transformation enables you to schedule the operation to run at specific times or based on specific events (such as new files being added to the source storage location).\n",
        "\n",
        "> **Tip:** For more information about using the **Stored procedure** activity in a pipeline, see [Transform data by using the SQL Server Stored Procedure activity in Azure Data Factory or Synapse Analytics](https://learn.microsoft.com/en-us/azure/data-factory/transform-data-using-stored-procedure) in the Azure Data Factory documentation.\n",
        "\n"
      ],
      "metadata": {
        "id": "8rA4A6jXjEoB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Next unit: Exercise - Transform files using a serverless SQL pool\n",
        "\n"
      ],
      "metadata": {
        "id": "4XTuEUXGjEmD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unit 5 of 7\n",
        "\n"
      ],
      "metadata": {
        "id": "-ixH4xh6jZqh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise - Transform files using a serverless SQL pool\n",
        "\n",
        "Now it's your opportunity to use the CREATE EXTERNAL TABLE AS SELECT statement to transform data. In this exercise, you'll use a provided script to provision an Azure Synapse Analytics workspace in your Azure subscription; and then use a serverless SQL pool to transform data from files in a data lake.\n",
        "\n",
        "> **Note:** To complete this lab, you will need an [Azure subscription](https://azure.microsoft.com/free) in which you have administrative access.\n",
        "\n",
        "Launch the exercise and follow the instructions.\n",
        "\n",
        "[Launch Exercise](https://aka.ms/mslearn-synapse-transform-sql)\n",
        "\n"
      ],
      "metadata": {
        "id": "DIZ53wCQjZoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Next unit: Knowledge check\n",
        "\n"
      ],
      "metadata": {
        "id": "B7knAq7UjZmG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unit  6 of 7\n",
        "\n"
      ],
      "metadata": {
        "id": "2t8WE9UOjZYj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Knowledge check\n",
        "\n",
        "1. You need to store the results of a query in a serverless SQL pool as files in a data lake. Which SQL statement should you use?\n",
        "\n",
        "    - ☐ BULK INSERT\n",
        "    - ☐ CREATE EXTERNAL TABLE AS SELECT\n",
        "    - ☑ COPY\n",
        "\n",
        "2. Which of the following file formats can you use to persist the results of a query?\n",
        "\n",
        "    - ☐ CSV only\n",
        "    - ☐ Parquet only.\n",
        "    - ☑ CSV and Parquet.\n",
        "\n",
        "3. You drop an existing external table from a database in a serverless SQL pool. What else must you do before recreating an external table with the same location?\n",
        "\n",
        "    - ☐ Delete the folder containing the data files for dropped table.\n",
        "    - ☐ Drop and recreate the database.\n",
        "    - ☑ Create an Apache Spark pool.\n",
        "\n"
      ],
      "metadata": {
        "id": "tXXFYsuvjZWd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Next unit: Summary\n",
        "\n"
      ],
      "metadata": {
        "id": "TzBBUjFfjZUl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unit 7 of 7\n",
        "\n"
      ],
      "metadata": {
        "id": "EA6h7bl2jZSc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "xa3AFy4WjEj0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "XQUN3bzsjj8M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "uWStkf8mjj2h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "HVa3K9a9jjNr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Hdcc0wLLjjLR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "GY9lm-TWjjJE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "GNGWlbU5jEhs"
      }
    }
  ]
}