{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# M02.01 Use Azure Synapse serverless SQL pool to query files in a data lake\n",
        "\n"
      ],
      "metadata": {
        "id": "qLugPdeOrHzK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unit 1 of 7\n",
        "\n"
      ],
      "metadata": {
        "id": "3mMLbQQNrLAL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Introduction\n",
        "\n",
        "Azure Synapse Analytics includes serverless SQL pools, which are tailored for querying data in a data lake. With a serverless SQL pool you can use SQL code to query data in files of various common formats without needing to load the file data into database storage. This capability helps data analysts and data engineers analyze and process file data in the data lake using a familiar data processing language, without the need to create or maintain a relational database store.\n",
        "\n",
        "After completing this module, you'll be able to:\n",
        "\n",
        "- Identify capabilities and use cases for serverless SQL pools in Azure Synapse Analytics\n",
        "- Query CSV, JSON, and Parquet files using a serverless SQL pool\n",
        "- Create external database objects in a serverless SQL pool\n",
        "\n"
      ],
      "metadata": {
        "id": "YxhjDDr8rK8_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prerequisites\n",
        "\n",
        "Before starting this module, you should have the following prerequisite skills and knowledge:\n",
        "\n",
        "- Familiarity with the Microsoft Azure portal\n",
        "- Familiarity with data lake and data warehouse concepts\n",
        "- Experience of using SQL to query database tables\n",
        "\n"
      ],
      "metadata": {
        "id": "j5KnsbzzrK6g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Next unit: Understand Azure Synapse serverless SQL pool capabilities and use cases\n",
        "\n"
      ],
      "metadata": {
        "id": "nKUeg1n_rK33"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unit 2 of 7\n",
        "\n"
      ],
      "metadata": {
        "id": "1RvZuJXbrK1n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Understand Azure Synapse serverless SQL pool capabilities and use cases\n",
        "\n",
        "Azure Synapse Analytics is an integrated analytics service that brings together a wide range of commonly used technologies for processing and analyzing data at scale. One of the most prevalent technologies used in data solutions is SQL - an industry standard language for querying and manipulating data.\n",
        "\n"
      ],
      "metadata": {
        "id": "jTX4t6iVrKy7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Serverless SQL pools in Azure Synapse Analytics\n",
        "\n",
        "Azure Synapse SQL is a distributed query system in Azure Synapse Analytics that offers two kinds of runtime environments:\n",
        "\n",
        "- **Serverless SQL pool:** on-demand SQL query processing, primarily used to work with data in a data lake.\n",
        "- **Dedicated SQL pool:** Enterprise-scale relational database instances used to host data warehouses in which data is stored in relational tables.\n",
        "\n",
        "In this module, we'll focus on serverless SQL pool, which provides a pay-per-query endpoint to query the data in your data lake. The benefits of using serverless SQL pool include:\n",
        "\n",
        "- A familiar Transact-SQL syntax to query data in place without the need to copy or load data into a specialized store.\n",
        "- Integrated connectivity from a wide range of business intelligence and ad-hoc querying tools, including the most popular drivers.\n",
        "- Distributed query processing that is built for large-scale data, and computational functions - resulting in fast query performance.\n",
        "- Built-in query execution fault-tolerance, resulting in high reliability and success rates even for long-running queries involving large data sets.\n",
        "- No infrastructure to setup or clusters to maintain. A built-in endpoint for this service is provided within every Azure Synapse workspace, so you can start querying data as soon as the workspace is created.\n",
        "- No charge for resources reserved, you're only charged for the data processed by queries you run.\n",
        "\n"
      ],
      "metadata": {
        "id": "Dq9ljWGerKwc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### When to use serverless SQL pools\n",
        "\n",
        "Serverless SQL pool is tailored for querying the data residing in the data lake, so in addition to eliminating management burden, it eliminates a need to worry about ingesting the data into the system. You just point the query to the data that is already in the lake and run it.\n",
        "\n",
        "Synapse SQL serverless resource model is great for unplanned or \"bursty\" workloads that can be processed using the always-on serverless SQL endpoint in your Azure Synapse Analytics workspace. Using the serverless pool helps when you need to know exact cost for each query executed to monitor and attribute costs.\n",
        "\n",
        "> **Note:** Serverless SQL pool is an analytics system and is not recommended for OLTP workloads such as databases used by applications to store transactional data. Workloads that require millisecond response times and are looking to pinpoint a single row in a data set are not good fit for serverless SQL pool.\n",
        "\n",
        "Common use cases for serverless SQL pools include:\n",
        "\n",
        "- **Data exploration:** Data exploration involves browsing the data lake to get initial insights about the data, and is easily achievable with Azure Synapse Studio. You can browse through the files in your linked data lake storage, and use the built-in serverless SQL pool to automatically generate a SQL script to select TOP 100 rows from a file or folder just as you would do with a table in SQL Server. From there, you can apply projections, filtering, grouping, and most of the operation over the data as if the data were in a regular SQL Server table.\n",
        "- **Data transformation:** While Azure Synapse Analytics provides great data transformations capabilities with Synapse Spark, some data engineers might find data transformation easier to achieve using SQL. Serverless SQL pool enables you to perform SQL-based data transformations; either interactively or as part of an automated data pipeline.\n",
        "- **Logical data warehouse:** After your initial exploration of the data in the data lake, you can define external objects such as tables and views in a serverless SQL database. The data remains stored in the data lake files, but are abstracted by a relational schema that can be used by client applications and analytical tools to query the data as they would in a relational database hosted in SQL Server.\n",
        "\n"
      ],
      "metadata": {
        "id": "h6O2zobirKuA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Next unit: Query files using a serverless SQL pool\n",
        "\n"
      ],
      "metadata": {
        "id": "XL8-r3UCrKrs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unit 3 of 7\n",
        "\n"
      ],
      "metadata": {
        "id": "I-N53JFZrKpX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Query files using a serverless SQL pool\n",
        "\n",
        "You can use a serverless SQL pool to query data files in various common file formats, including:\n",
        "\n",
        "- Delimited text, such as comma-separated values (CSV) files.\n",
        "- JavaScript object notation (JSON) files.\n",
        "- Parquet files.\n",
        "\n",
        "The basic syntax for querying is the same for all of these types of file, and is built on the OPENROWSET SQL function; which generates a tabular rowset from data in one or more files. For example, the following query could be used to extract data from CSV files.\n",
        "\n",
        "```sql\n",
        "SELECT TOP 100 *\n",
        "FROM OPENROWSET(\n",
        "    BULK 'https://mydatalake.blob.core.windows.net/data/files/*.csv',\n",
        "    FORMAT = 'csv') AS rows\n",
        "```\n",
        "\n",
        "The OPENROWSET function includes more parameters that determine factors such as:\n",
        "\n",
        "- The schema of the resulting rowset\n",
        "- Additional formatting options for delimited text files.\n",
        "\n",
        "> **Tip:** You'll find the full syntax for the OPENROWSET function in the [Azure Synapse Analytics documentation](https://learn.microsoft.com/en-us/azure/synapse-analytics/sql/develop-openrowset#syntax).\n",
        "\n",
        "The output from OPENROWSET is a rowset to which an alias must be assigned. In the previous example, the alias **rows** is used to name the resulting rowset.\n",
        "\n",
        "The **BULK** parameter includes the full URL to the location in the data lake containing the data files. This can be an individual file, or a folder with a wildcard expression to filter the file types that should be included. The **FORMAT** parameter specifies the type of data being queried. The example above reads delimited text from all .csv files in the **files** folder.\n",
        "\n",
        "> **Note:** This example assumes that the user has access to the files in the underlying store, If the files are protected with a SAS key or custom identity, you would need to [create a server-scoped credential](https://learn.microsoft.com/en-us/azure/synapse-analytics/sql/develop-storage-files-storage-access-control?tabs=shared-access-signature#server-scoped-credential).\n",
        "\n",
        "As seen in the previous example, you can use wildcards in the BULK parameter to include or exclude files in the query. The following list shows a few examples of how this can be used:\n",
        "\n",
        "- `https://mydatalake.blob.core.windows.net/data/files/file1.csv`: Only include *file1.csv* in the *files* folder.\n",
        "- `https://mydatalake.blob.core.windows.net/data/files/file*.csv`: All .csv files in the *files* folder with names that start with \"file\".\n",
        "- `https://mydatalake.blob.core.windows.net/data/files/*`: All files in the *files* folder.\n",
        "- `https://mydatalake.blob.core.windows.net/data/files/**`: All files in the *files* folder, and recursively its subfolders.\n",
        "\n",
        "You can also specify multiple file paths in the **BULK** parameter, separating each path with a comma.\n",
        "\n"
      ],
      "metadata": {
        "id": "yMk-rHJlrKm3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Querying delimited text files\n",
        "\n",
        "Delimited text files are a common file format within many businesses. The specific formatting used in delimited files can vary, for example:\n",
        "\n",
        "- With and without a header row.\n",
        "- Comma and tab-delimited values.\n",
        "- Windows and Unix style line endings.\n",
        "- Non-quoted and quoted values, and escaping characters.\n",
        "\n",
        "Regardless of the type of delimited file you're using, you can read data from them by using the OPENROWSET function with the **csv** FORMAT parameter, and other parameters as required to handle the specific formatting details for your data. For example:\n",
        "\n",
        "```sql\n",
        "SELECT TOP 100 *\n",
        "FROM OPENROWSET(\n",
        "    BULK 'https://mydatalake.blob.core.windows.net/data/files/*.csv',\n",
        "    FORMAT = 'csv',\n",
        "    PARSER_VERSION = '2.0',\n",
        "    FIRSTROW = 2) AS rows\n",
        "```\n",
        "\n",
        "The **PARSER_VERSION** is used to determine how the query interprets the text encoding used in the files. Version 1.0 is the default and supports a wide range of file encodings, while version 2.0 supports fewer encodings but offers better performance. The **FIRSTROW** parameter is used to skip rows in the text file, to eliminate any unstructured preamble text or to ignore a row containing column headings.\n",
        "\n",
        "Additional parameters you might require when working with delimited text files include:\n",
        "\n",
        "- FIELDTERMINATOR - the character used to separate field values in each row. For example, a tab-delimited file separates fields with a TAB (*\\t*) character. The default field terminator is a comma (*,*).\n",
        "- ROWTERMINATOR - the character used to signify the end of a row of data. For example, a standard Windows text file uses a combination of a carriage return (CR) and line feed (LF), which is indicated by the code *\\n*; while UNIX-style text files use a single line feed character, which can be indicated using the code *0x0a*.\n",
        "- FIELDQUOTE - the character used to enclose quoted string values. For example, to ensure that the comma in the address field value *126 Main St, apt 2* isn't interpreted as a field delimiter, you might enclose the entire field value in quotation marks like this: *\"126 Main St, apt 2\"*. The double-quote (\") is the default field quote character.\n",
        "\n",
        "> **Tip:** For details of additional parameters when working with delimited text files, refer to the [Azure Synapse Analytics documentation](https://learn.microsoft.com/en-us/azure/synapse-analytics/sql/develop-openrowset#syntax).\n",
        "\n"
      ],
      "metadata": {
        "id": "HpFo83TYrKkj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Specifying the rowset schema\n",
        "\n",
        "It's common for delimited text files to include the column names in the first row. The OPENROWSET function can use this to define the schema for the resulting rowset, and automatically infer the data types of the columns based on the values they contain. For example, consider the following delimited text:\n",
        "\n",
        "```text\n",
        "product_id,product_name,list_price\n",
        "123,Widget,12.99\n",
        "124,Gadget,3.99\n",
        "```\n",
        "\n",
        "The data consists of the following three columns:\n",
        "\n",
        "- **product_id** (integer number)\n",
        "- **product_name** (string)\n",
        "- **list_price** (decimal number)\n",
        "\n",
        "You could use the following query to extract the data with the correct column names and appropriately inferred SQL Server data types (in this case INT, NVARCHAR, and DECIMAL)\n",
        "\n",
        "```sql\n",
        "SELECT TOP 100 *\n",
        "FROM OPENROWSET(\n",
        "    BULK 'https://mydatalake.blob.core.windows.net/data/files/*.csv',\n",
        "    FORMAT = 'csv',\n",
        "    PARSER_VERSION = '2.0',\n",
        "    HEADER_ROW = TRUE) AS rows\n",
        "```\n",
        "\n",
        "The **HEADER_ROW** parameter (which is only available when using parser version 2.0) instructs the query engine to use the first row of data in each file as the column names, like this:\n",
        "\n",
        "| product_id | product_name | list_price |\n",
        "| :-- | :-- | :-- |\n",
        "| 123 | Widget | 12.9900 |\n",
        "| 124 | Gadget | 3.9900 |\n",
        "\n",
        "Now consider the following data:\n",
        "\n",
        "```text\n",
        "123,Widget,12.99\n",
        "124,Gadget,3.99\n",
        "```\n",
        "\n",
        "This time, the file doesn't contain the column names in a header row; so while the data types can still be inferred, the column names will be set to **C1**, **C2**, **C3**, and so on.\n",
        "\n",
        "| C1 | C2 | C3 |\n",
        "| :-- | :-- | :-- |\n",
        "| 123 | Widget | 12.9900 |\n",
        "| 124 | Gadget | 3.9900 |\n",
        "\n",
        "To specify explicit column names and data types, you can override the default column names and inferred data types by providing a schema definition in a **WITH** clause, like this:\n",
        "\n",
        "```sql\n",
        "SELECT TOP 100 *\n",
        "FROM OPENROWSET(\n",
        "    BULK 'https://mydatalake.blob.core.windows.net/data/files/*.csv',\n",
        "    FORMAT = 'csv',\n",
        "    PARSER_VERSION = '2.0')\n",
        "WITH (\n",
        "    product_id INT,\n",
        "    product_name VARCHAR(20) COLLATE Latin1_General_100_BIN2_UTF8,\n",
        "    list_price DECIMAL(5,2)\n",
        ") AS rows\n",
        "```\n",
        "\n",
        "This query produces the expected results:\n",
        "\n",
        "| product_id | product_name | list_price |\n",
        "| :-- | :-- | :-- |\n",
        "| 123 | Widget | 12.99 |\n",
        "| 124 | Gadget | 3.99 |\n",
        "\n",
        "> Tip: When working with text files, you may encounter some incompatibility with UTF-8 encoded data and the collation used in the **master** database for the serverless SQL pool. To overcome this, you can specify a compatible collation for individual VARCHAR columns in the schema. See the [troubleshooting guidance](https://learn.microsoft.com/en-us/azure/synapse-analytics/troubleshoot/reading-utf8-text) for more details.\n",
        "\n"
      ],
      "metadata": {
        "id": "CpA0KlpKrKh_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Querying JSON files\n",
        "\n",
        "JSON is a popular format for web applications that exchange data through REST interfaces or use NoSQL data stores such as Azure Cosmos DB. So, it's not uncommon to persist data as JSON documents in files in a data lake for analysis.\n",
        "\n",
        "For example, a JSON file that defines an individual product might look like this:\n",
        "\n",
        "```json\n",
        "{\n",
        "    \"product_id\": 123,\n",
        "    \"product_name\": \"Widget\",\n",
        "    \"list_price\": 12.99\n",
        "}\n",
        "```\n",
        "\n",
        "To return product data from a folder containing multiple JSON files in this format, you could use the following SQL query:\n",
        "\n",
        "```sql\n",
        "SELECT doc\n",
        "FROM\n",
        "    OPENROWSET(\n",
        "        BULK 'https://mydatalake.blob.core.windows.net/data/files/*.json',\n",
        "        FORMAT = 'csv',\n",
        "        FIELDTERMINATOR ='0x0b',\n",
        "        FIELDQUOTE = '0x0b',\n",
        "        ROWTERMINATOR = '0x0b'\n",
        "    ) WITH (doc NVARCHAR(MAX)) as rows\n",
        "```\n",
        "\n",
        "OPENROWSET has no specific format for JSON files, so you must use *csv* format with **FIELDTERMINATOR**, **FIELDQUOTE**, and **ROWTERMINATOR** set to *0x0b*, and a schema that includes a single NVARCHAR(MAX) column. The result of this query is a rowset containing a single column of JSON documents, like this:\n",
        "\n",
        "| doc |\n",
        "| :-- |\n",
        "| {\"product_id\":123,\"product_name\":\"Widget\",\"list_price\": 12.99} |\n",
        "| {\"product_id\":124,\"product_name\":\"Gadget\",\"list_price\": 3.99} |\n",
        "\n",
        "To extract individual values from the JSON, you can use the JSON_VALUE function in the SELECT statement, as shown here:\n",
        "\n",
        "```sql\n",
        "SELECT JSON_VALUE(doc, '$.product_name') AS product,\n",
        "           JSON_VALUE(doc, '$.list_price') AS price\n",
        "FROM\n",
        "    OPENROWSET(\n",
        "        BULK 'https://mydatalake.blob.core.windows.net/data/files/*.json',\n",
        "        FORMAT = 'csv',\n",
        "        FIELDTERMINATOR ='0x0b',\n",
        "        FIELDQUOTE = '0x0b',\n",
        "        ROWTERMINATOR = '0x0b'\n",
        "    ) WITH (doc NVARCHAR(MAX)) as rows\n",
        "```\n",
        "\n",
        "This query would return a rowset similar to the following results:\n",
        "\n",
        "| product | price |\n",
        "| :-- | :-- |\n",
        "| Widget | 12.99 |\n",
        "| Gadget |3.99 |\n",
        "\n"
      ],
      "metadata": {
        "id": "zXTG_ArJrKfv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Querying Parquet files\n",
        "\n",
        "Parquet is a commonly used format for big data processing on distributed file storage. It's an efficient data format that is optimized for compression and analytical querying.\n",
        "\n",
        "In most cases, the schema of the data is embedded within the Parquet file, so you only need to specify the **BULK** parameter with a path to the file(s) you want to read, and a **FORMAT** parameter of *parquet*; like this:\n",
        "\n",
        "```sql\n",
        "SELECT TOP 100 *\n",
        "FROM OPENROWSET(\n",
        "    BULK 'https://mydatalake.blob.core.windows.net/data/files/*.*',\n",
        "    FORMAT = 'parquet') AS rows\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "ODiWkWDmrKdv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Query partitioned data\n",
        "\n",
        "It's common in a data lake to partition data by splitting across multiple files in subfolders that reflect partitioning criteria. This enables distributed processing systems to work in parallel on multiple partitions of the data, or to easily eliminate data reads from specific folders based on filtering criteria. For example, suppose you need to efficiently process sales order data, and often need to filter based on the year and month in which orders were placed. You could partition the data using folders, like this:\n",
        "\n",
        "- /orders\n",
        "  - /year=2020\n",
        "    - /month=1\n",
        "      - /01012020.parquet\n",
        "      - /02012020.parquet\n",
        "      - ...\n",
        "    - /month=2\n",
        "      - /01022020.parquet\n",
        "      - /02022020.parquet\n",
        "      - ...\n",
        "    - ...\n",
        "  - /year=2021\n",
        "    - /month=1\n",
        "      - /01012021.parquet\n",
        "      - /02012021.parquet\n",
        "      - ...\n",
        "  - ...\n",
        "\n",
        "To create a query that filters the results to include only the orders for January and February 2020, you could use the following code:\n",
        "\n",
        "```sql\n",
        "SELECT *\n",
        "FROM OPENROWSET(\n",
        "    BULK 'https://mydatalake.blob.core.windows.net/data/orders/year=*/month=*/*.*',\n",
        "    FORMAT = 'parquet') AS orders\n",
        "WHERE orders.filepath(1) = '2020'\n",
        "    AND orders.filepath(2) IN ('1','2');\n",
        "```\n",
        "\n",
        "The numbered filepath parameters in the WHERE clause reference the wildcards in the folder names in the BULK path -so the parameter 1 is the \\* in the year=\\* folder name, and parameter 2 is the \\* in the month=\\* folder name.\n",
        "\n"
      ],
      "metadata": {
        "id": "WFpGeMmmrKbL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Next unit: Create external database objects\n",
        "\n"
      ],
      "metadata": {
        "id": "72sVM804rKYj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unit 4 of 7\n",
        "\n"
      ],
      "metadata": {
        "id": "UMwOXSAJrKWU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create external database objects\n",
        "\n",
        "You can use the OPENROWSET function in SQL queries that run in the default **master** database of the built-in serverless SQL pool to explore data in the data lake. However, sometimes you may want to create a custom database that contains some objects that make it easier to work with external data in the data lake that you need to query frequently.\n",
        "\n"
      ],
      "metadata": {
        "id": "M3goMK4XrKT_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Creating a database\n",
        "\n",
        "You can create a database in a serverless SQL pool just as you would in a SQL Server instance. You can use the graphical interface in Synapse Studio, or a CREATE DATABASE statement. One consideration is to set the collation of your database so that it supports conversion of text data in files to appropriate Transact-SQL data types.\n",
        "\n",
        "The following example code creates a database named *salesDB* with a collation that makes it easier to import UTF-8 encoded text data into VARCHAR columns.\n",
        "\n",
        "```sql\n",
        "CREATE DATABASE SalesDB\n",
        "    COLLATE Latin1_General_100_BIN2_UTF8\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "mLhYu33crKRg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Creating an external data source\n",
        "\n",
        "You can use the OPENROWSET function with a BULK path to query file data from your own database, just as you can in the **master** database; but if you plan to query data in the same location frequently, it's more efficient to define an external data source that references that location. For example, the following code creates a data source named *files* for the hypothetical `https://mydatalake.blob.core.windows.net/data/files/` folder:\n",
        "\n",
        "```sql\n",
        "CREATE EXTERNAL DATA SOURCE files\n",
        "WITH (\n",
        "    LOCATION = 'https://mydatalake.blob.core.windows.net/data/files/'\n",
        ")\n",
        "```\n",
        "\n",
        "One benefit of an external data source, is that you can simplify an OPENROWSET query to use the combination of the data source and the relative path to the folders or files you want to query:\n",
        "\n",
        "```sql\n",
        "SELECT *\n",
        "FROM\n",
        "    OPENROWSET(\n",
        "        BULK 'orders/*.csv',\n",
        "        DATA_SOURCE = 'files',\n",
        "        FORMAT = 'csv',\n",
        "        PARSER_VERSION = '2.0'\n",
        "    ) AS orders\n",
        "```\n",
        "\n",
        "In this example, the **BULK** parameter is used to specify the relative path for all .csv files in the **orders** folder, which is a subfolder of the **files** folder referenced by the data source.\n",
        "\n",
        "Another benefit of using a data source is that you can assign a credential for the data source to use when accessing the underlying storage, enabling you to provide access to data through SQL without permitting users to access the data directly in the storage account. For example, the following code creates a credential that uses a shared access signature (SAS) to authenticate against the underlying Azure storage account hosting the data lake.\n",
        "\n",
        "```sql\n",
        "CREATE DATABASE SCOPED CREDENTIAL sqlcred\n",
        "WITH\n",
        "    IDENTITY='SHARED ACCESS SIGNATURE',  \n",
        "    SECRET = 'sv=xxx...';\n",
        "GO\n",
        "\n",
        "CREATE EXTERNAL DATA SOURCE secureFiles\n",
        "WITH (\n",
        "    LOCATION = 'https://mydatalake.blob.core.windows.net/data/secureFiles/'\n",
        "    CREDENTIAL = sqlcred\n",
        ");\n",
        "GO\n",
        "```\n",
        "\n",
        "> **Tip:** In addition to SAS authentication, you can define credentials that use *managed identity* (the Microsoft Entra identity used by your Azure Synapse workspace), a specific Microsoft Entra principal, or passthrough authentication based on the identity of the user running the query (which is the default type of authentication). To learn more about using credentials in a serverless SQL pool, see the [Control storage account access for serverless SQL pool in Azure Synapse Analytics](https://learn.microsoft.com/en-us/azure/synapse-analytics/sql/develop-storage-files-storage-access-control) article in Azure Synapse Analytics documentation.\n",
        "\n"
      ],
      "metadata": {
        "id": "SUm14S5xrKO_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Creating an external file format\n",
        "\n",
        "While an external data source simplifies the code needed to access files with the OPENROWSET function, you still need to provide format details for the file being access; which may include multiple settings for delimited text files. You can encapsulate these settings in an external file format, like this:\n",
        "\n",
        "```sql\n",
        "CREATE EXTERNAL FILE FORMAT CsvFormat\n",
        "    WITH (\n",
        "        FORMAT_TYPE = DELIMITEDTEXT,\n",
        "        FORMAT_OPTIONS(\n",
        "            FIELD_TERMINATOR = ',',\n",
        "            STRING_DELIMITER = '\"'\n",
        "        )\n",
        "    );\n",
        "GO\n",
        "```\n",
        "\n",
        "After creating file formats for the specific data files you need to work with, you can use the file format to create external tables, as discussed next.\n",
        "\n"
      ],
      "metadata": {
        "id": "6BKyXnOXrKMb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Creating an external table\n",
        "\n",
        "When you need to perform a lot of analysis or reporting from files in the data lake, using the OPENROWSET function can result in complex code that includes data sources and file paths. To simplify access to the data, you can encapsulate the files in an external table; which users and reporting applications can query using a standard SQL SELECT statement just like any other database table. To create an external table, use the CREATE EXTERNAL TABLE statement, specifying the column schema as for a standard table, and including a WITH clause specifying the external data source, relative path, and external file format for your data.\n",
        "\n",
        "```sql\n",
        "CREATE EXTERNAL TABLE dbo.products\n",
        "(\n",
        "    product_id INT,\n",
        "    product_name VARCHAR(20),\n",
        "    list_price DECIMAL(5,2)\n",
        ")\n",
        "WITH\n",
        "(\n",
        "    DATA_SOURCE = files,\n",
        "    LOCATION = 'products/*.csv',\n",
        "    FILE_FORMAT = CsvFormat\n",
        ");\n",
        "GO\n",
        "\n",
        "-- query the table\n",
        "SELECT * FROM dbo.products;\n",
        "```\n",
        "\n",
        "By creating a database that contains the external objects discussed in this unit, you can provide a relational database layer over files in a data lake, making it easier for many data analysts and reporting tools to access the data by using standard SQL query semantics.\n",
        "\n"
      ],
      "metadata": {
        "id": "Ekud5yg_rKJ7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Next unit: Exercise - Query files using a serverless SQL pool\n",
        "\n"
      ],
      "metadata": {
        "id": "yjPRYpuSrKG_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unit 5 of 7\n",
        "\n"
      ],
      "metadata": {
        "id": "pAnGb-AyrKEs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise - Query files using a serverless SQL pool\n",
        "\n",
        "Now it's your opportunity to try using a serverless SQL pool for yourself. In this exercise, you'll use a provided script to provision an Azure Synapse Analytics workspace in your Azure subscription; and then use a serverless SQL pool to query data files in a data lake.\n",
        "\n",
        "> **Note:** To complete this lab, you will need an [Azure subscription](https://azure.microsoft.com/free) in which you have administrative access.\n",
        "\n",
        "Launch the exercise and follow the instructions.\n",
        "\n",
        "[Launch Exercise](https://aka.ms/mslearn-synapse-sql)\n",
        "\n"
      ],
      "metadata": {
        "id": "9mzDKgf8rKCE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Next unit: Knowledge check\n",
        "\n"
      ],
      "metadata": {
        "id": "MAnQ6x_erJ_j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unit 6 of 7\n",
        "\n"
      ],
      "metadata": {
        "id": "03rOmY_srJ9D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Knowledge check\n",
        "\n",
        "1. What function is used to read the data in files stored in a data lake?\n",
        "    - ☐ FORMAT\n",
        "    - ☐ ROWSET\n",
        "    - ☑ OPENROWSET\n",
        "    > Correct. The OPENROWSET is used to read the data in files stored in a data lake.\n",
        "\n",
        "2. What character in file path can be used to select all the file/folders that match rest of the path?\n",
        "\n",
        "    - ☐ &\n",
        "    - ☑ \\*\n",
        "    > Correct. The asterisk character in file path can be used to select all the file or folders that match rest of the path.\n",
        "    - ☐ /\n",
        "\n",
        "3. Which external database object encapsulates the connection information to a file location in a data lake store?\n",
        "\n",
        "    - ☐ FILE FORMAT\n",
        "    - ☑ DATA SOURCE\n",
        "    > Correct. a DATA SOURCE provides the connection information to the files in a data lake store.\n",
        "    - ☐ EXTERNAL TABLE\n",
        "\n"
      ],
      "metadata": {
        "id": "a5ujBFhPrJ6z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Next unit: Summary\n",
        "\n"
      ],
      "metadata": {
        "id": "ttn_iZ-ArJ4b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unit 7 of 7\n",
        "\n"
      ],
      "metadata": {
        "id": "EyYuEy7mrJ2X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Summary\n",
        "\n",
        "Serverless SQL pools enable you to easily query files in data lake. You can query various file formats CSV, JSON, Parquet, and create external database objects to provide a relational abstraction layer over the raw files.\n",
        "\n",
        "In this module, you've learned how to:\n",
        "\n",
        "- Identify capabilities and use cases for serverless SQL pools in Azure Synapse Analytics\n",
        "- Query CSV, JSON, and Parquet files using a serverless SQL pool\n",
        "- Create external database objects in a serverless SQL pool\n",
        "\n"
      ],
      "metadata": {
        "id": "KgWWa_cRrJy7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Learn more\n",
        "\n",
        "To learn more about using serverless SQL pools to query files, refer to the [Azure Synapse Analytics documentation](https://learn.microsoft.com/en-us/azure/synapse-analytics/sql/on-demand-workspace-overview).\n",
        "\n"
      ],
      "metadata": {
        "id": "w9RyTgM5rJwg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### All units complete\n",
        "\n"
      ],
      "metadata": {
        "id": "XmDtOYNurJuD"
      }
    }
  ]
}