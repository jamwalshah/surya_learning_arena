{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Spark Structured Streaming and Delta Tables\n",
        "\n",
        "Spark provides support for streaming data through *Spark Structured Streaming* and extends this support through *delta tables* that can be targets (*sinks*) or *sources* of streaming data.\n",
        "\n",
        "In this exercise, you'll use Spark to ingest a stream of data from a folder of JSON files that consists of simulated status messages from devices. In a real scenario, the data could come from some other real-time source, such as a Kafka queue or an Azure Event Hub.\n",
        "\n",
        "## Create a folder for the incoming stream of data\n",
        "\n",
        "1. Ensure this notebook is attached to your Spark pool (using this **Attach to** drop-down list above).\n",
        "2. Run the cell below to create a folder named **data** to which the simulated device data will be written.\n",
        "\n",
        "    > **Note**: The first cell may take some time to run because the Spark pool must be started.\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from notebookutils import mssparkutils\n",
        "\n",
        "# Create a folder\n",
        "inputPath = '/data/'\n",
        "mssparkutils.fs.mkdirs(inputPath)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "sparkpool0110",
              "statement_id": 2,
              "statement_ids": [
                2
              ],
              "livy_statement_state": "available",
              "spark_jobs": null,
              "session_id": "0",
              "state": "finished",
              "normalized_state": "finished",
              "queued_time": "2024-08-08T17:10:11.2686033Z",
              "session_start_time": "2024-08-08T17:10:11.3238631Z",
              "execution_start_time": "2024-08-08T17:13:25.9040032Z",
              "execution_finish_time": "2024-08-08T17:13:26.9903362Z",
              "parent_msg_id": "cc3b0aa7-7d15-42fc-b6b5-d5aca39bfdc8"
            },
            "text/plain": "StatementMeta(sparkpool0110, 0, 2, Finished, Available, Finished)"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "execution_count": 5,
          "data": {
            "text/plain": "True"
          },
          "metadata": {}
        }
      ],
      "execution_count": 1,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Use Spark Structured Streaming to query a stream of data\n",
        "\n",
        "1. Run the cell below to create a streaming dataframe that reads data from the folder based on a JSON schema that includes the name of the device and its status."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "# Create a stream that reads data from the folder, using a JSON schema\n",
        "jsonSchema = StructType([\n",
        "  StructField(\"device\", StringType(), False),\n",
        "  StructField(\"status\", StringType(), False)\n",
        "])\n",
        "\n",
        "fileDF = spark.readStream.schema(jsonSchema).option(\"maxFilesPerTrigger\", 1).json(inputPath)\n",
        "\n"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "sparkpool0110",
              "statement_id": 3,
              "statement_ids": [
                3
              ],
              "livy_statement_state": "available",
              "spark_jobs": null,
              "session_id": "0",
              "state": "finished",
              "normalized_state": "finished",
              "queued_time": "2024-08-08T17:13:38.7265757Z",
              "session_start_time": null,
              "execution_start_time": "2024-08-08T17:13:38.8998243Z",
              "execution_finish_time": "2024-08-08T17:13:41.6986444Z",
              "parent_msg_id": "c1e75665-be4b-4bb7-a8d3-52c700e03424"
            },
            "text/plain": "StatementMeta(sparkpool0110, 0, 3, Finished, Available, Finished)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 2,
      "metadata": {
        "microsoft": {},
        "collapsed": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Wait for the cell above to complete.\n",
        "3. When the streaming dataframe has been created, you can apply a transformation query to aggregate the data and write the results to an output stream. Run the following code to filter the incoming stream for errors in the device data, and count the number of errors per device."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "countDF = fileDF.filter(\"status == 'error'\").groupBy(\"device\").count()\n",
        "query = countDF.writeStream.format(\"memory\").queryName(\"counts\").outputMode(\"complete\").start()\n",
        "print('Streaming query started.')"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "sparkpool0110",
              "statement_id": 4,
              "statement_ids": [
                4
              ],
              "livy_statement_state": "available",
              "spark_jobs": null,
              "session_id": "0",
              "state": "finished",
              "normalized_state": "finished",
              "queued_time": "2024-08-08T17:16:08.5996723Z",
              "session_start_time": null,
              "execution_start_time": "2024-08-08T17:16:08.7354223Z",
              "execution_finish_time": "2024-08-08T17:16:12.6985503Z",
              "parent_msg_id": "5f7befba-6fd0-4a4b-8826-4c5f6b640588"
            },
            "text/plain": "StatementMeta(sparkpool0110, 0, 4, Finished, Available, Finished)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Streaming query started.\n"
          ]
        }
      ],
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. The query output is streamed to an in-memory table. Run the cell below to use SQL to query this table and veiw the number of errors per device."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%sql\n",
        "select * from counts\n"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "sparkpool0110",
              "statement_id": 6,
              "statement_ids": [
                6
              ],
              "livy_statement_state": "available",
              "spark_jobs": null,
              "session_id": "0",
              "state": "finished",
              "normalized_state": "finished",
              "queued_time": "2024-08-08T17:17:04.6228282Z",
              "session_start_time": null,
              "execution_start_time": "2024-08-08T17:17:04.7666886Z",
              "execution_finish_time": "2024-08-08T17:17:05.82251Z",
              "parent_msg_id": "e6f6b658-6956-4ce7-9a7c-9c950062ac7f"
            },
            "text/plain": "StatementMeta(sparkpool0110, 0, 6, Finished, Available, Finished)"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "execution_count": 5,
          "data": {
            "application/vnd.synapse.sparksql-result+json": {
              "schema": {
                "type": "struct",
                "fields": [
                  {
                    "name": "device",
                    "type": "string",
                    "nullable": true,
                    "metadata": {}
                  },
                  {
                    "name": "count",
                    "type": "long",
                    "nullable": false,
                    "metadata": {}
                  }
                ]
              },
              "data": []
            },
            "text/plain": "<Spark SQL result set with 0 rows and 2 fields>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 5,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "microsoft": {
          "language": "sparksql"
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Note that the query returns no data, because we haven't written any device status data there yet.\n",
        "6. Let's fix that by writing some status event data from a couple of simulated devices."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device_data = '''{\"device\":\"Dev1\",\"status\":\"ok\"}\n",
        "{\"device\":\"Dev1\",\"status\":\"ok\"}\n",
        "{\"device\":\"Dev1\",\"status\":\"ok\"}\n",
        "{\"device\":\"Dev2\",\"status\":\"error\"}\n",
        "{\"device\":\"Dev1\",\"status\":\"ok\"}\n",
        "{\"device\":\"Dev1\",\"status\":\"error\"}\n",
        "{\"device\":\"Dev2\",\"status\":\"ok\"}\n",
        "{\"device\":\"Dev2\",\"status\":\"error\"}\n",
        "{\"device\":\"Dev1\",\"status\":\"ok\"}'''\n",
        "\n",
        "mssparkutils.fs.put(inputPath + \"data.txt\", device_data, True)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "sparkpool0110",
              "statement_id": 7,
              "statement_ids": [
                7
              ],
              "livy_statement_state": "available",
              "spark_jobs": null,
              "session_id": "0",
              "state": "finished",
              "normalized_state": "finished",
              "queued_time": "2024-08-08T17:17:44.0023766Z",
              "session_start_time": null,
              "execution_start_time": "2024-08-08T17:17:44.1497885Z",
              "execution_finish_time": "2024-08-08T17:17:44.673545Z",
              "parent_msg_id": "9be3cb08-b836-4cd2-bdfe-da27e54c17be"
            },
            "text/plain": "StatementMeta(sparkpool0110, 0, 7, Finished, Available, Finished)"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "execution_count": 11,
          "data": {
            "text/plain": "True"
          },
          "metadata": {}
        }
      ],
      "execution_count": 6,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Run the SQL query again to see the aggregated error counts (if the query still returns no data, wait a few seconds and try again!) There should be one error for device 1, and two errors for device 2."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%sql\n",
        "select * from counts\n"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "sparkpool0110",
              "statement_id": 10,
              "statement_ids": [
                10
              ],
              "livy_statement_state": "available",
              "spark_jobs": null,
              "session_id": "0",
              "state": "finished",
              "normalized_state": "finished",
              "queued_time": "2024-08-08T17:18:41.6580075Z",
              "session_start_time": null,
              "execution_start_time": "2024-08-08T17:18:41.7829401Z",
              "execution_finish_time": "2024-08-08T17:18:42.8475367Z",
              "parent_msg_id": "46a4d8f2-c544-4fde-bdda-ae599e471c3d"
            },
            "text/plain": "StatementMeta(sparkpool0110, 0, 10, Finished, Available, Finished)"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "execution_count": 9,
          "data": {
            "application/vnd.synapse.sparksql-result+json": {
              "schema": {
                "type": "struct",
                "fields": [
                  {
                    "name": "device",
                    "type": "string",
                    "nullable": true,
                    "metadata": {}
                  },
                  {
                    "name": "count",
                    "type": "long",
                    "nullable": false,
                    "metadata": {}
                  }
                ]
              },
              "data": [
                [
                  "Dev2",
                  "2"
                ],
                [
                  "Dev1",
                  "1"
                ]
              ]
            },
            "text/plain": "<Spark SQL result set with 2 rows and 2 fields>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 9,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "microsoft": {
          "language": "sparksql"
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Review the results, noting the number of errors. Then run the following code to write more device data."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "more_data = '''{\"device\":\"Dev1\",\"status\":\"ok\"}\n",
        "{\"device\":\"Dev1\",\"status\":\"ok\"}\n",
        "{\"device\":\"Dev1\",\"status\":\"ok\"}\n",
        "{\"device\":\"Dev1\",\"status\":\"ok\"}\n",
        "{\"device\":\"Dev1\",\"status\":\"error\"}\n",
        "{\"device\":\"Dev2\",\"status\":\"error\"}\n",
        "{\"device\":\"Dev1\",\"status\":\"ok\"}'''\n",
        "\n",
        "mssparkutils.fs.put(inputPath + \"more-data.txt\", more_data, True)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "sparkpool0110",
              "statement_id": 11,
              "statement_ids": [
                11
              ],
              "livy_statement_state": "available",
              "spark_jobs": null,
              "session_id": "0",
              "state": "finished",
              "normalized_state": "finished",
              "queued_time": "2024-08-08T17:18:48.9327079Z",
              "session_start_time": null,
              "execution_start_time": "2024-08-08T17:18:49.0809788Z",
              "execution_finish_time": "2024-08-08T17:18:49.6137243Z",
              "parent_msg_id": "88385b3c-59e2-4171-96ab-0a8b312fd463"
            },
            "text/plain": "StatementMeta(sparkpool0110, 0, 11, Finished, Available, Finished)"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "execution_count": 13,
          "data": {
            "text/plain": "True"
          },
          "metadata": {}
        }
      ],
      "execution_count": 10,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Run the SQL query again (waiting a few seconds if necessary) to see the new status events reflected in the aggregations. There should now be two errors for device 1, and three errors for device 2."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%sql\n",
        "select * from counts\n"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "sparkpool0110",
              "statement_id": 12,
              "statement_ids": [
                12
              ],
              "livy_statement_state": "available",
              "spark_jobs": null,
              "session_id": "0",
              "state": "finished",
              "normalized_state": "finished",
              "queued_time": "2024-08-08T17:18:52.4739911Z",
              "session_start_time": null,
              "execution_start_time": "2024-08-08T17:18:52.5930339Z",
              "execution_finish_time": "2024-08-08T17:18:53.6572068Z",
              "parent_msg_id": "0c57c08e-21ed-4a68-916a-9b2f3817766a"
            },
            "text/plain": "StatementMeta(sparkpool0110, 0, 12, Finished, Available, Finished)"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "execution_count": 11,
          "data": {
            "application/vnd.synapse.sparksql-result+json": {
              "schema": {
                "type": "struct",
                "fields": [
                  {
                    "name": "device",
                    "type": "string",
                    "nullable": true,
                    "metadata": {}
                  },
                  {
                    "name": "count",
                    "type": "long",
                    "nullable": false,
                    "metadata": {}
                  }
                ]
              },
              "data": [
                [
                  "Dev2",
                  "2"
                ],
                [
                  "Dev1",
                  "1"
                ]
              ]
            },
            "text/plain": "<Spark SQL result set with 2 rows and 2 fields>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 11,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "microsoft": {
          "language": "sparksql"
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create a delta table\n",
        "\n",
        "Azure Synapse Analytics supports the Linux Foundation *Delta Lake* architecture, which builds on Spark Structured Streaming to add support for transactions, versioning, and other useful capabilities.\n",
        "\n",
        "In particular, you can create *delta tables* as a target (or *sink*) for streaming data, or as a *source* of streaming data for downstream queries.\n",
        "\n",
        "To explore this, we'll write the streaming dataframe based on the **data** folder we created previously to a new delta table, which we'll define using a path to a location in the file system.\n",
        "\n",
        "1. Run the cell below to stream the folder data to a delta table."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "delta_table_path = inputPath + 'deltatable'\n",
        "stream = fileDF.writeStream.format(\"delta\").option(\"checkpointLocation\", inputPath + 'checkpoint').start(delta_table_path)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "sparkpool0110",
              "statement_id": 13,
              "statement_ids": [
                13
              ],
              "livy_statement_state": "available",
              "spark_jobs": null,
              "session_id": "0",
              "state": "finished",
              "normalized_state": "finished",
              "queued_time": "2024-08-08T17:21:34.3839396Z",
              "session_start_time": null,
              "execution_start_time": "2024-08-08T17:21:34.8359871Z",
              "execution_finish_time": "2024-08-08T17:21:36.3917524Z",
              "parent_msg_id": "0ca4b71e-2f77-424d-8367-ce0b86b05e35"
            },
            "text/plain": "StatementMeta(sparkpool0110, 0, 13, Finished, Available, Finished)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 12,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Now run the next cell to query the delta table to see the data that has been streamed to it. If at first the query returns no data, wait a few seconds and run the cell again)."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.format(\"delta\").load(delta_table_path)\n",
        "display(df)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "sparkpool0110",
              "statement_id": 14,
              "statement_ids": [
                14
              ],
              "livy_statement_state": "available",
              "spark_jobs": null,
              "session_id": "0",
              "state": "finished",
              "normalized_state": "finished",
              "queued_time": "2024-08-08T17:22:35.8103211Z",
              "session_start_time": null,
              "execution_start_time": "2024-08-08T17:22:35.9507537Z",
              "execution_finish_time": "2024-08-08T17:22:41.4314665Z",
              "parent_msg_id": "97372a56-aa57-4d10-afc6-abed55deff98"
            },
            "text/plain": "StatementMeta(sparkpool0110, 0, 14, Finished, Available, Finished)"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.synapse.widget-view+json": {
              "widget_id": "a0b2dc92-abb5-457d-a2d5-03536f1e5eaa",
              "widget_type": "Synapse.DataFrame"
            },
            "text/plain": "SynapseWidget(Synapse.DataFrame, a0b2dc92-abb5-457d-a2d5-03536f1e5eaa)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 13,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Delta tables enable you to use a feature named *time travel* to view the data at a previous point in time.\n",
        "\n",
        "4. Run the following query to retrieve the initial micro-batch of data that was streamed from the file data. "
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(delta_table_path)\n",
        "display(df)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "sparkpool0110",
              "statement_id": 15,
              "statement_ids": [
                15
              ],
              "livy_statement_state": "available",
              "spark_jobs": null,
              "session_id": "0",
              "state": "finished",
              "normalized_state": "finished",
              "queued_time": "2024-08-08T17:23:36.45774Z",
              "session_start_time": null,
              "execution_start_time": "2024-08-08T17:23:36.5987097Z",
              "execution_finish_time": "2024-08-08T17:23:41.9087926Z",
              "parent_msg_id": "2a23c54a-4685-4dbf-a5c0-4f7a18dfdd01"
            },
            "text/plain": "StatementMeta(sparkpool0110, 0, 15, Finished, Available, Finished)"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.synapse.widget-view+json": {
              "widget_id": "864bceaf-3348-4181-a93f-19241ef21999",
              "widget_type": "Synapse.DataFrame"
            },
            "text/plain": "SynapseWidget(Synapse.DataFrame, 864bceaf-3348-4181-a93f-19241ef21999)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 14,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Now that you've finished exploring Spark Structured Streaming and delta tables, stop the stream of data and clean up the files used in this exercise."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stream.stop()\n",
        "query.stop()\n",
        "print(\"Stream stopped\")\n",
        "mssparkutils.fs.rm(inputPath, True)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "sparkpool0110",
              "statement_id": 16,
              "statement_ids": [
                16
              ],
              "livy_statement_state": "available",
              "spark_jobs": null,
              "session_id": "0",
              "state": "finished",
              "normalized_state": "finished",
              "queued_time": "2024-08-08T17:25:01.2566991Z",
              "session_start_time": null,
              "execution_start_time": "2024-08-08T17:25:01.433299Z",
              "execution_finish_time": "2024-08-08T17:25:01.9540076Z",
              "parent_msg_id": "b154ab69-648d-4468-882d-102cb17a345b"
            },
            "text/plain": "StatementMeta(sparkpool0110, 0, 16, Finished, Available, Finished)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stream stopped\n"
          ]
        },
        {
          "output_type": "execute_result",
          "execution_count": 21,
          "data": {
            "text/plain": "True"
          },
          "metadata": {}
        }
      ],
      "execution_count": 15,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "language": "Python",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "description": "Run by Surya",
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {
        "a0b2dc92-abb5-457d-a2d5-03536f1e5eaa": {
          "type": "Synapse.DataFrame",
          "sync_state": {
            "table": {
              "rows": [
                {
                  "0": "Dev1",
                  "1": "ok"
                },
                {
                  "0": "Dev1",
                  "1": "ok"
                },
                {
                  "0": "Dev1",
                  "1": "ok"
                },
                {
                  "0": "Dev2",
                  "1": "error"
                },
                {
                  "0": "Dev1",
                  "1": "ok"
                },
                {
                  "0": "Dev1",
                  "1": "error"
                },
                {
                  "0": "Dev2",
                  "1": "ok"
                },
                {
                  "0": "Dev2",
                  "1": "error"
                },
                {
                  "0": "Dev1",
                  "1": "ok"
                },
                {
                  "0": "Dev1",
                  "1": "ok"
                },
                {
                  "0": "Dev1",
                  "1": "ok"
                },
                {
                  "0": "Dev1",
                  "1": "ok"
                },
                {
                  "0": "Dev1",
                  "1": "ok"
                },
                {
                  "0": "Dev1",
                  "1": "error"
                },
                {
                  "0": "Dev2",
                  "1": "error"
                },
                {
                  "0": "Dev1",
                  "1": "ok"
                }
              ],
              "schema": [
                {
                  "key": "0",
                  "name": "device",
                  "type": "string"
                },
                {
                  "key": "1",
                  "name": "status",
                  "type": "string"
                }
              ],
              "truncated": false
            },
            "isSummary": false,
            "language": "scala"
          },
          "persist_state": {
            "view": {
              "type": "details",
              "tableOptions": {},
              "chartOptions": {
                "chartType": "bar",
                "aggregationType": "count",
                "categoryFieldKeys": [
                  "0"
                ],
                "seriesFieldKeys": [
                  "0"
                ],
                "isStacked": false
              }
            }
          }
        },
        "864bceaf-3348-4181-a93f-19241ef21999": {
          "type": "Synapse.DataFrame",
          "sync_state": {
            "table": {
              "rows": [
                {
                  "0": "Dev1",
                  "1": "ok"
                },
                {
                  "0": "Dev1",
                  "1": "ok"
                },
                {
                  "0": "Dev1",
                  "1": "ok"
                },
                {
                  "0": "Dev2",
                  "1": "error"
                },
                {
                  "0": "Dev1",
                  "1": "ok"
                },
                {
                  "0": "Dev1",
                  "1": "error"
                },
                {
                  "0": "Dev2",
                  "1": "ok"
                },
                {
                  "0": "Dev2",
                  "1": "error"
                },
                {
                  "0": "Dev1",
                  "1": "ok"
                }
              ],
              "schema": [
                {
                  "key": "0",
                  "name": "device",
                  "type": "string"
                },
                {
                  "key": "1",
                  "name": "status",
                  "type": "string"
                }
              ],
              "truncated": false
            },
            "isSummary": false,
            "language": "scala"
          },
          "persist_state": {
            "view": {
              "type": "details",
              "tableOptions": {},
              "chartOptions": {
                "chartType": "bar",
                "aggregationType": "count",
                "categoryFieldKeys": [
                  "0"
                ],
                "seriesFieldKeys": [
                  "0"
                ],
                "isStacked": false
              }
            }
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}